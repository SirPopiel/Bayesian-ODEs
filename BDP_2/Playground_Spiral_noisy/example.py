import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

import numpy as np
import numpy.random as npr

import tensorflow as tf
#import tensorflow.compat.v1 as tf
#tf.disable_v2_behavior()

import matplotlib.pyplot as plt
plt.switch_backend('agg')
import tensorflow.python.eager as tfe
# c'è problema di compatibilità con tf python eager

from tensorflow.keras import layers, initializers
from scipy.integrate import odeint


keras = tf.keras
tf.compat.v1.enable_eager_execution()

from neural_ode import NeuralODE

np.random.seed(1234)
tf.compat.v1.set_random_seed(1234)

if __name__ == "__main__":

    # Definition of the systetm dynamic
    def Spiral(z, t, alpha, beta, gamma, theta):
        x, y = z
        dzdt = [alpha * x**3 + beta * y**3, gamma * x**3 + theta * y**3]
        return dzdt


    # Plot trajectories in 2D state space
    def plot_spiral(trajectories, width = 1.):
        x = trajectories[:,0]
        y = trajectories[:,1]
        plt.plot(x, y, linewidth=width)


    # Set of training data generated by simulating the exact dynamics of equation
    data_size = 1001
    batch_time = 80
    niters = 1500 # N iteration for HMC
    batch_size = 920


    # True model parameters
    alpha = -0.1
    beta = 2.0
    gamma = -2.0
    theta = -0.1

    z0 = [2., 0.] # initial condition
    true_y0 = tf.cast([[2., 0.]], dtype=tf.float32) # change of type of z0
    t_grid = np.linspace(0, 20, data_size) # time grid

    true_yy = odeint(Spiral, z0, t_grid, args=(alpha, beta, gamma, theta)) 
    true_yy.shape # exact values solving the ODE system

    true_yy.shape
    true_y = true_yy + 0.02 * np.random.randn(true_yy.shape[0], true_yy.shape[1]) # samples from a
    # gaussian wiyh parameters mu = true_yy and sigma = 0.02
    # data with addition of a gaussian noise (mean 0 and sigma 0.02)
 

    def get_batch():
        """Returns initial point and last point over sampled frament of trajectory"""
        starts = np.random.choice(np.arange(data_size - batch_time - 1, dtype=np.int64), batch_size, replace=False)
        batch_y0 = true_y[starts] 
        batch_yN = true_y[starts + batch_time]
        return tf.cast(batch_y0, dtype=tf.float32), tf.cast(batch_yN, dtype=tf.float32)
        # see LV_noisy for comment of the lines above


    Order = 4 # we need 4 for the Order of the equation, so that range(1, Order) goes from 1 to 3 (order)
    num_param = 1
    for m in range(1, Order):
        for n in range(m + 1):
            num_param += 1
            # we are counting the tot number of parameters (1 for order0 + 2 for order1 + 3 for order2...)
            
    num_param = num_param * 2 # we multiply by 2
    # (see matrix A of parameters, we have num_param in the first row + num_param in the second row)


    t0 = t_grid[:batch_time][0]
    t1 = t_grid[:batch_time][-1]
    t_in = np.linspace(t0, t1, 20)
    # see lines 88-92 in LV_noisy


    batch_y0, batch_yN = get_batch()

    #########################################
    ########## precondition start ###########
    #########################################
    niters_pre = 1000

    class ODEModel_pre(tf.keras.Model):
        def __init__(self):
            super(ODEModel_pre, self).__init__()
            self.Weights = tf.Variable(tf.random.normal([num_param//2, 2], dtype=tf.float32)*0.01, dtype=tf.float32)
            # same comment line 103 LV_noisy

        def call(self, inputs, **kwargs):
            t, y = inputs
            h = y #**3   # batch_size x 2
            x1 = h[:,0:1]
            x2 = h[:,1:2]
            temp = x1 ** 0 
            for m in range(1, Order):
                for n in range(m + 1):
                    temp = tf.concat([temp, x1**n * x2**(m - n)], 1)
                    
            h_out = tf.matmul(temp, self.Weights) # system evaluated in the input data 'inputs'
            # [x, t] using the weights of the model
            return h_out


    model_pre = ODEModel_pre()
    neural_ode_pre = NeuralODE(model_pre, t_in)
    optimizer = tf.compat.v1.train.AdamOptimizer(3e-2)
    # see lines 124-127 LV_noisy

    #@tfe.defun
    @tf.function
    def compute_gradients_and_update_pre(batch_y0, batch_yN):
        """Takes start positions (x0, y0) and final positions (xN, yN)"""
        pred_y = neural_ode_pre.forward(batch_y0) # for the preconditioner: prediction of y using Runge-Kutta 4 method using as inputs
        # the first y observed in each batch (y0 in batch_y0)
        with tf.GradientTape() as g_pre:
            g_pre.watch(pred_y)
            # Loss function and then gradient of the Loss f.
            loss = tf.reduce_mean(input_tensor=(pred_y - batch_yN)**2) + tf.reduce_sum(input_tensor=tf.abs(model_pre.Weights))
            
        dLoss = g_pre.gradient(loss, pred_y)
        h_start, dfdh0, dWeights = neural_ode_pre.backward(pred_y, dLoss) # derivative of the weights (gradient descent applied)
        optimizer.apply_gradients(zip(dWeights, model_pre.weights)) # weights update
        return loss, dWeights

    # Compile EAGER graph to static (this will be much faster)
    # compute_gradients_and_update_pre = tfe.defun(compute_gradients_and_update_pre)

    parameters_pre = np.zeros((niters_pre, num_param))

    for step in range(niters_pre):
        print(step)
        loss, dWeights = compute_gradients_and_update_pre(batch_y0, batch_yN)

        model_parameters_pre = model_pre.trainable_weights[0].numpy().flatten()
        for k in range(num_param):
            parameters_pre[step, k] = model_parameters_pre[k]
        
        print(parameters_pre[step,:])

    #########################################
    ########## precondition end #############
    #########################################

    initial_weight = model_pre.trainable_weights[0].numpy() # initialization thanks to preconditioning procedure up to now
    print(initial_weight.shape, "here")

    class ODEModel(tf.keras.Model):
        def __init__(self, initial_weight):
            super(ODEModel, self).__init__()
            self.Weights = tf.Variable(initial_weight, dtype=tf.float32)

        def call(self, inputs, **kwargs):
            t, y = inputs
            h = y
            x1 = h[:,0:1]
            x2 = h[:,1:2]
            temp = x1 ** 0 
            for m in range(1, Order):
                for n in range(m + 1):
                    temp = tf.concat([temp, x1**n * x2**(m - n)], 1)
                    
            h_out = tf.matmul(temp, self.Weights)
            return h_out
        # same procedure as for preconditioning steps


    model = ODEModel(initial_weight) # model initialized with the weights resulting from the preconditioning
    neural_ode = NeuralODE(model, t = t_in)

    #@tfe.defun
    @tf.function
    def compute_gradients_and_update(batch_y0, batch_yN): 
        """Takes start positions (x0, y0) and final positions (xN, yN)"""
        pred_y = neural_ode.forward(batch_y0) # y prediction for each y0 in batch y0 (see above as for the preconditioning procedure)
        with tf.GradientTape() as g:
            g.watch(pred_y)
            loss = tf.reduce_sum(input_tensor=(pred_y - batch_yN)**2)
            
        dLoss = g.gradient(loss, pred_y)
        h_start, dfdh0, dWeights = neural_ode.backward(pred_y, dLoss) # derivative of the weights (gradient descent applied)

        return loss, dWeights

    # Compile EAGER graph to static (this will be much faster)
    # compute_gradients_and_update = tfe.defun(compute_gradients_and_update)



    # Bayesian techniques introduced:



    # function to compute the kinetic energy
    # def kinetic_energy(V, loggamma_v, loglambda_v):
    #     q = (np.sum(-V**2) - loggamma_v**2 - loglambda_v**2)/2.0
    #     return q

    def kinetic_energy(V, loggamma_v, loglambda_v):
        q = (np.sum(-V**2)/mom_theta - loggamma_v**2/mom_gamma - loglambda_v**2/mom_lambda)/2.0
        return q

    # def compute_gradient_param(dWeights, loggamma, loglambda, batch_size, para_num):
    #     WW = model.trainable_weights[0].numpy()
    #     dWeights = np.exp(loggamma)/2.0 * dWeights + np.exp(loglambda) * np.sign(WW)
    #     return dWeights

    def compute_gradient_param(dWeights, loggamma, loglambda, batch_size, para_num):
        WW = model.trainable_weights[0].numpy()
        dWeights = np.exp(loggamma)/2.0 * dWeights + np.exp(loglambda) * np.sign(WW - w_means)
        return dWeights

    # def compute_gradient_hyper(loss, weights, loggamma, loglambda, batch_size, para_num):
    #     grad_loggamma = np.exp(loggamma) * (loss/2.0 + 1.0) - (batch_size/2.0 + 1.0)
    #     grad_loglambda = np.exp(loglambda) * (np.sum(np.abs(weights)) + 1.0) - (para_num + 1.0)
    #     # gradient of the hyperparameters for the update in the following
    #
    #     return grad_loggamma, grad_loglambda

    def compute_gradient_hyper(loss, weights, loggamma, loglambda, batch_size, para_num):
        grad_loggamma = np.exp(loggamma) * (loss/2.0 + 1.0) - (batch_size/2.0 + 1.0)
        grad_loglambda = np.exp(loglambda) * (np.sum(np.abs(weights - w_means)) + 1.0) - (para_num + 1.0)
        # gradient of the hyperparameters for the update in the following

        return grad_loggamma, grad_loglambda

    # def compute_Hamiltonian(loss, weights, loggamma, loglambda, batch_size, para_num):
    #     H = np.exp(loggamma)*(loss/2.0 + 1.0) + np.exp(loglambda)*(np.sum(np.abs(weights)) + 1.0)\
    #              - (batch_size/2.0 + 1.0) * loggamma - (para_num + 1.0) * loglambda
    #     return H

    def compute_Hamiltonian(loss, weights, loggamma, loglambda, batch_size, para_num):
        H = np.exp(loggamma)*(loss/2.0 + 1.0) + np.exp(loglambda)*(np.sum(np.abs(weights - w_means)) + 1.0)\
                 - (batch_size/2.0 + 1.0) * loggamma - (para_num + 1.0) * loglambda
        return H

    def leap_frog(v_in, w_in, loggamma_in, loglambda_in, loggamma_v_in, loglambda_v_in): # scheme for the integration of the dynamical
        # system (of the gradient of the Hamiltonian) to construct the Markov Chain and update the hyperp., parameters and weights
        # assign weights from the previous step to the model
        model.trainable_weights[0].assign(w_in)
        
        print(model.trainable_weights)

        v_new = v_in
        loggamma_v_new = loggamma_v_in
        loglambda_v_new = loglambda_v_in

        loggamma_new = loggamma_in
        loglambda_new = loglambda_in
        w_new = w_in

        for m in range(L):

            loss, dWeights = compute_gradients_and_update(batch_y0, batch_yN) # evaluate the gradient
            print(loss)

            dWeights = np.asarray(dWeights[0]) # make the gradient to be numpy array
            dWeights = compute_gradient_param(dWeights, loggamma_new, loglambda_new, batch_size, num_param)
            grad_loggamma, grad_loglambda = compute_gradient_hyper(loss, w_new, loggamma_new, loglambda_new, batch_size, num_param)

            loggamma_v_new = loggamma_v_new - epsilon/2*grad_loggamma
            loglambda_v_new = loglambda_v_new - epsilon/2*grad_loglambda
            v_new = v_new - epsilon/2*(dWeights)
            w_new = model.trainable_weights[0].numpy() + epsilon * v_new
            model.trainable_weights[0].assign(w_new)
            loggamma_new = loggamma_new + epsilon/mom_gamma * loggamma_v_new
            loglambda_new = loglambda_new + epsilon/mom_lambda * loglambda_v_new
            
            # Second half of the leap frog
            loss, dWeights = compute_gradients_and_update(batch_y0, batch_yN)
            dWeights = np.asarray(dWeights[0])
            dWeights = compute_gradient_param(dWeights, loggamma_new, loglambda_new, batch_size, num_param)
            grad_loggamma, grad_loglambda = compute_gradient_hyper(loss, w_new, loggamma_new, loglambda_new, batch_size, num_param)

            v_new = v_new - epsilon/2*(dWeights)
            loggamma_v_new = loggamma_v_new - epsilon/2*grad_loggamma
            loglambda_v_new = loglambda_v_new - epsilon/2*grad_loglambda

        print(dWeights)
        print(np.exp(loggamma_new))
        print(np.exp(loglambda_new))

        return v_new, w_new, loggamma_new, loglambda_new, loggamma_v_new, loglambda_v_new




    neural_ode_test = NeuralODE(model, t=t_grid)
    parameters = np.zeros((niters, num_param//2, 2)) # book keeping the parameters
    loggammalist = np.zeros((niters, 1)) # book keeping the loggamma
    loglambdalist = np.zeros((niters, 1)) # book keeping the loggamma
    loglikelihood = np.zeros((niters, 1)) # book keeping the loggamma
    mom_theta = 0.15  # Momentum for the Hamiltonian Monte Carlo for theta parameters
    mom_lambda = 0.01  # Momentum for the Hamiltonian Monte Carlo for lambda parameters
    mom_gamma = 0.01  # Momentum for the Hamiltonian Monte Carlo for gamma parameters
    L = 40 # leap frog step number
    epsilon = 0.002 # leap frog step size
    epsilon_max = 0.0002    # max 0.001
    epsilon_min = 0.0002   # max 0.001
    acc_rate = 0  # Used to compute and display the acceptance rate of the candidates
    w_means = np.zeros(num_param, dtype=np.float32)  # A priori means of parameters of the model
    w_means = w_means.reshape(10, 2)
    

    def compute_epsilon(step):
        coefficient = np.log(epsilon_max/epsilon_min)
        return epsilon_max * np.exp( - step * coefficient / niters) # needed for leap frog scheme


    # initial weight
    w_temp = initial_weight # choice of weights from the preconditioning step
    print("initial_w", w_temp)
    loggamma_temp = 4. + np.random.normal()
    loglambda_temp = np.random.normal()

    model.trainable_weights[0].assign(w_temp) # weights assigned to the model
    loss_original, _ = compute_gradients_and_update(batch_y0, batch_yN) # compute the initial Hamiltonian

    loggamma_temp = np.log(batch_size / loss_original) # precision of the Gaussian noise distribution gamma (see page 8/22 paper)
    print("This is initial guess", loggamma_temp, "with loss", loss_original)
    if loggamma_temp > 6.:
        loggamma_temp = 6.
        epsilon_max = 0.0001
        epsilon_min = 0.0001



    # training steps
    for step in range(niters):

        epsilon = compute_epsilon(step)

        print(step)

        v_initial = np.sqrt(mom_theta)*np.random.randn(num_param//2, 2) # initialize the velocity
        loggamma_v_initial = np.sqrt(mom_gamma)*np.random.normal()
        loglambda_v_initial = np.sqrt(mom_lambda)*np.random.normal()

        loss_initial, _ = compute_gradients_and_update(batch_y0, batch_yN) # compute the initial Hamiltonian
        loss_initial = compute_Hamiltonian(loss_initial, w_temp, loggamma_temp, loglambda_temp, batch_size, num_param) # initial
        # loss with weights resulting from preconditioner

        v_new, w_new, loggamma_new, loglambda_new, loggamma_v_new, loglambda_v_new = \
                                leap_frog(v_initial, w_temp, loggamma_temp, loglambda_temp, loggamma_v_initial, loglambda_v_initial)
        # updating the model (hyper)parameters and the weights

        # compute the final Hamiltonian
        loss_finial, _ = compute_gradients_and_update(batch_y0, batch_yN)
        loss_finial = compute_Hamiltonian(loss_finial, w_new, loggamma_new, loglambda_new, batch_size, num_param) # final loss
        # with weights resulting from the leap frog optimization

        # making decisions
        p_temp = np.exp(-loss_finial + loss_initial + \
                        kinetic_energy(v_new, loggamma_v_new, loglambda_v_new) - kinetic_energy(v_initial, loggamma_v_initial, loglambda_v_initial))

        p = min(1, p_temp)
        p_decision = np.random.uniform()
        if p > p_decision:
            parameters[step:step+1, :, :] = w_new # the updated weights from leap frog
            # all the updated parameters
            w_temp = w_new
            loggammalist[step, 0] = loggamma_new
            loglambdalist[step, 0] = loglambda_new
            loglikelihood[step, 0] = loss_finial
            loggamma_temp = loggamma_new
            loglambda_temp = loglambda_new
            acc_rate += 1
        else:
            parameters[step:step+1, :, :] = w_temp # the weights from preconditioning
            # all the non updated parameters
            model.trainable_weights[0].assign(w_temp)
            loggammalist[step, 0] = loggamma_temp
            loglambdalist[step, 0] = loglambda_temp
            loglikelihood[step, 0] = loss_initial


        print('probability', p)
        print(p > p_decision)

    print('Acceptance rate: ', acc_rate / niters)
        
    np.save('parameters', parameters) # parameters chain
    np.save('loggammalist', loggammalist) # loggamma chain
    np.save('loglikelihood', loglikelihood) # likelihood chain

    np.savetxt("data_weights.csv", parameters.reshape((1500, 20)), delimiter=',')
    np.savetxt("data_loggammalist.csv", loggammalist, delimiter=',')
    np.savetxt("data_loglikelihood.csv", loglikelihood, delimiter=',')
    np.savetxt("data_loglambda.csv", loglambdalist, delimiter=',')
