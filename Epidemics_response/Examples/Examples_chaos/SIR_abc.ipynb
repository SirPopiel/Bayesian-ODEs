{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "#plt.switch_backend('agg')\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "keras = tf.keras\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "#from neural_ode import NeuralODE\n",
    "\n",
    "import sys\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "target_dir = os.path.sep.join(current_dir.split(os.path.sep)[:])\n",
    "sys.path.append(os.path.join(os.path.dirname(target_dir),'../eABCSMC'))\n",
    "sys.path.append(os.path.join(os.path.dirname(target_dir),'../Epidemics_response/Data'))\n",
    "sys.path.append(target_dir)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR(z, t, beta, gamma):\n",
    "\n",
    "    S, I, R = z\n",
    "\n",
    "    dS = - beta * I * S / N \n",
    "    dI = beta * I * S / N - gamma * I\n",
    "    dR = gamma * I \n",
    "\n",
    "    dzdt = [dS, dI, dR]\n",
    "    return dzdt\n",
    "\n",
    "def SIR_diffparam(z, t, beta, beta2, gamma, gamma2, errato = 0):\n",
    "\n",
    "    S, I, R = z\n",
    "\n",
    "    dS = - beta * I * S / N \n",
    "    dI = beta2 * I * S / N - gamma * I\n",
    "    dR = gamma2 * I \n",
    "\n",
    "    dzdt = [dS, dI, dR]\n",
    "    return dzdt\n",
    "\n",
    "# adattare il modello tempo-variabile -> così da considerare lockdown ecc? -> o con alcuni threshold\n",
    "\n",
    "def plot_traj(trajectories, width = 1.):\n",
    "    x1 = trajectories[:,0]\n",
    "    x2 = trajectories[:,1]\n",
    "    x3 = trajectories[:,2]\n",
    "    s = plt.plot(x1, linewidth = width, label = 'Susceptible')\n",
    "    i = plt.plot(x2, linewidth = width, label = 'Infected')\n",
    "    r = plt.plot(x3, linewidth = width, label = 'Removed')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.title('Simulation with R0 = %f' %R0 )\n",
    "    plt.savefig('SIR.pdf', dpi=300)\n",
    "    \n",
    "def plot_traj_gray(trajectories, width = 1.):\n",
    "    x1 = trajectories[:,0]\n",
    "    x2 = trajectories[:,1]\n",
    "    x3 = trajectories[:,2]\n",
    "    s = plt.plot(x1, linewidth = width, color = 'lightgray')\n",
    "    i = plt.plot(x2, linewidth = width, color = 'lightgray')\n",
    "    r = plt.plot(x3, linewidth = width, color = 'lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oop con ereditarietà?\n",
    "\n",
    "def SEIR(z, t, beta, gamma, lamb, mu, a):\n",
    "\n",
    "    S, E, I, R = z\n",
    "\n",
    "    dS = - beta * I * S / N \n",
    "    dE = beta * I * S / N - a * E\n",
    "    dI = a * E - gamma  * I\n",
    "    dR = gamma * I \n",
    "\n",
    "    dzdt = [dS, dE, dI, dR]\n",
    "    return dzdt\n",
    "\n",
    "def plot_traj_SEIR(trajectories, width = 1.):\n",
    "    x1 = trajectories[:,0]\n",
    "    x2 = trajectories[:,1]\n",
    "    x3 = trajectories[:,2]\n",
    "    x4 = trajectories[:,3]\n",
    "    \n",
    "    s = plt.plot(x1, linewidth = width, label = 'Susceptible')\n",
    "    r = plt.plot(x2, linewidth = width, label = 'Exposed')\n",
    "    i = plt.plot(x3, linewidth = width, label = 'Infected')\n",
    "    r = plt.plot(x4, linewidth = width, label = 'Removed')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.title('Simulation with R0 = %f' %(a*beta/gamma) )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 100\n",
    "\n",
    "batch_time = 20  # tempo delle batches\n",
    "niters = 500\n",
    "#batch_size = 84  # dimensione delle batches\n",
    "batch_size = data_size - batch_time - 1\n",
    "\n",
    "N = 60000\n",
    "infected_0 = 100\n",
    "beta = 0.3 # farli time evolving?\n",
    "gamma = 0.1\n",
    "\n",
    "R0 = beta/gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_grid = np.linspace(0, data_size-1, data_size)  # uniformly spaced data? -> even though advantage is learning with not uniformly spaced data\n",
    "z0 = [N - infected_0, infected_0, 0] # initial conditions\n",
    "true_yy = odeint(SIR, z0, t_grid, args=(beta, gamma))  # potrebbe aver senso tenerli in memoria se è lento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj(true_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y = true_yy.copy()\n",
    "\n",
    "\n",
    "\n",
    "from stochastic.processes.continuous import FractionalBrownianMotion\n",
    "\n",
    "fbm = FractionalBrownianMotion(hurst=0.2, t= data_size)\n",
    "# in questo modo è autocorrelato negativamente -> giorno da tanti tamponi seguito da giorno da pochi tamponi\n",
    "\n",
    "noise = abs(N/200 * fbm.sample(data_size-1))\n",
    "noise_2 = abs(N/200 * fbm.sample(data_size-1))\n",
    "\n",
    "\n",
    "true_y[:,0] = abs(true_y[:,0] - noise)        \n",
    "true_y[:,1] = abs(true_y[:,1] + noise - noise_2)\n",
    "true_y[:,2] = N - true_y[:,0] - true_y[:,1]\n",
    "\n",
    "# problema in realtà è che i suscettibili e i rimossi possono crescere/decrescere\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_y = true_yy\n",
    "plot_traj(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_s = np.std(true_y[:,0:1])  \n",
    "sigma_i = np.std(true_y[:,1:2]) \n",
    "sigma_r = np.std(true_y[:,2:3]) \n",
    "\n",
    "true_y[:, 0:1] = true_y[:, 0:1]/sigma_s\n",
    "true_y[:, 1:2] = true_y[:, 1:2]/sigma_i\n",
    "true_y[:, 2:3] = true_y[:, 2:3]/sigma_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    \"\"\"Returns initial point and last point over sampled frament of trajectory\"\"\"\n",
    "    starts = np.random.choice(np.arange(data_size - batch_time - 1, dtype=np.int64), batch_size, replace=False)\n",
    "    # This randomly chooses from {0, 1, ... , data_size - batch_time - 1}, batch_size different elements\n",
    "    batch_y0 = true_y[starts] \n",
    "    batch_yN = true_y[starts + batch_time]\n",
    "    # The function returns a tensor composed by some y0 and the respective yN,\n",
    "    # being y0 + DeltaT.\n",
    "    return tf.cast(batch_y0, dtype=tf.float32), tf.cast(batch_yN, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_param = 2 # Number of parameters\n",
    "para_num = num_param\n",
    "\n",
    "t0 = t_grid[:batch_time][0]  # t0 = first element of t_grid\n",
    "t1 = t_grid[:batch_time][-1]  # t1 = the element of t_grid at batch_time\n",
    "t_in = np.linspace(t0, t1, 10)  # The time grid between t0 and t1\n",
    "\n",
    "batch_y0, batch_yN = get_batch()  # Returns the first and the last y observed for each batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_ode import NeuralODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters_pre = 500  # Number of iterations of the preconditioner\n",
    "\n",
    "class ODEModel_pre(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ODEModel_pre, self).__init__()\n",
    "        self.Weights = tf.Variable(tf.random.normal([num_param, 1], dtype=tf.float32)*0.01, dtype=tf.float32)\n",
    "    # Initializer: assign normally distributed random weights which are very close to zero\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        t, y = inputs\n",
    "        h = y\n",
    "        S = h[:, 0:1] * sigma_s\n",
    "        I = h[:, 1:2] * sigma_i\n",
    "        R = h[:, 2:3] * sigma_r\n",
    "        \n",
    "\n",
    "        # ma che sintassi di merda -> facciamolo adattativo\n",
    "\n",
    "        p1 = self.Weights[0]\n",
    "        p2 = self.Weights[1]\n",
    "\n",
    "        dS = - p1 * I * S / N \n",
    "        dI = p1 * I * S / N - p2 * I \n",
    "        dR = p2 * I \n",
    "        \n",
    "        h_out = tf.concat([dS/sigma_s, dI/sigma_i, dR/sigma_r], 1)\n",
    "        return h_out\n",
    "\n",
    "\n",
    "model_pre = ODEModel_pre()  \n",
    "neural_ode_pre = NeuralODE(model_pre, t_in) \n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(3e-2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_gradients_and_update_pre(batch_y0, batch_yN):\n",
    "    \"\"\"Takes start positions (x0, y0) and final positions (xN, yN)\"\"\"\n",
    "    pred_y = neural_ode_pre.forward(batch_y0)  # Predict y using Runge-Kutta 4 for each y0 in batch_y0\n",
    "    with tf.GradientTape() as g_pre:\n",
    "        g_pre.watch(pred_y)\n",
    "        loss = tf.reduce_mean(input_tensor=(pred_y - batch_yN)**2) + tf.reduce_sum(input_tensor=tf.abs(model_pre.trainable_weights[0]))\n",
    "        # This step is computing the loss function\n",
    "    dLoss = g_pre.gradient(loss, pred_y)  # Here we compute the gradient of the loss function\n",
    "    h_start, dfdh0, dWeights = neural_ode_pre.backward(pred_y, dLoss)  # Here we compute the dWeights\n",
    "    optimizer.apply_gradients(zip(dWeights, model_pre.weights))  # Here we update the weights\n",
    "    return loss, dWeights\n",
    "\n",
    "#parameters_pre = np.zeros((para_num, niters_pre))\n",
    "parameters_pre = np.zeros((para_num, 1))\n",
    "\n",
    "for step in tqdm(range(niters_pre)):\n",
    "    loss, dWeights = compute_gradients_and_update_pre(batch_y0, batch_yN)\n",
    "    #parameters_pre[:,step] = np.reshape(model_pre.trainable_weights[0].numpy(),(5,))\n",
    "    parameters_pre = model_pre.trainable_weights[0].numpy()\n",
    "print(parameters_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weight = parameters_pre  # We initialize the weights with the parameters found in preconditioning\n",
    "print(initial_weight.shape, \"here\")\n",
    "\n",
    "\n",
    "class ODEModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ODEModel, self).__init__()\n",
    "        self.Weights = tf.Variable(tf.random.normal([num_param, 1], dtype=tf.float32)*0.01, dtype=tf.float32)\n",
    "        # Initializer, initializes the weight to normal random variables with sd = 0.01\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        t, y = inputs\n",
    "        h = y\n",
    "        S = h[:, 0:1] * sigma_s\n",
    "        I = h[:, 1:2] * sigma_i\n",
    "        R = h[:, 2:3] * sigma_r\n",
    "        \n",
    "\n",
    "        # ma che sintassi di merda -> facciamolo adattativo\n",
    "\n",
    "        p1 = self.Weights[0]\n",
    "        p2 = self.Weights[1]\n",
    "        \n",
    "        dS = - p1 * I * S / N \n",
    "        dI = p1 * I * S / N - p2 * I \n",
    "        dR = p2 * I \n",
    "        \n",
    "        h_out = tf.concat([dS/sigma_s, dI/sigma_i, dR/sigma_r], 1)\n",
    "        return h_out\n",
    "\n",
    "\n",
    "model = ODEModel()\n",
    "neural_ode = NeuralODE(model, t=t_in)  # We assign to NeuralODE the just created model and the time grid  between t0 and t1\n",
    "\n",
    "temp_model = ODEModel()\n",
    "neural_ode_temp = NeuralODE(temp_model, t=t_in)\n",
    "\n",
    "# qua facciamo un temp neuralode con tempmodel\n",
    "\n",
    "@tf.function\n",
    "def compute_gradients_and_update(batch_y0, batch_yN): \n",
    "    \"\"\"Takes start positions (x0, y0) and final positions (xN, yN)\"\"\"\n",
    "    pred_y = neural_ode.forward(batch_y0)  # This finds the predicted yNs\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(pred_y)\n",
    "        loss = tf.reduce_sum(input_tensor=(pred_y - batch_yN)**2)  # This creates the loss function\n",
    "\n",
    "    dLoss = g.gradient(loss, pred_y)  # This computes the gradient of the loss function\n",
    "    h_start, dfdh0, dWeights = neural_ode.backward(pred_y, dLoss)  # This applies the gradient descent to find\n",
    "    # the updates for the weights\n",
    "\n",
    "    return loss, dWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(batch_y0, batch_yN, neuralodes):\n",
    "    pred_y = neuralodes.forward(batch_y0)  # This finds the predicted yNs\n",
    "    loss = tf.reduce_mean(input_tensor=(pred_y - batch_yN)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_temp = np.resize([0.32,0.11],(2,1))\n",
    "temp_model.trainable_weights[0].assign(w_temp) \n",
    "compute_loss(batch_y0,batch_yN,neural_ode_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "\n",
    "# DA QUI IN POI NUOVO\n",
    "\n",
    "# Versione 1.0\n",
    "\n",
    "# Qui sotto parte una bozza di implementazione del metodo ABC come descritto su\n",
    "# https://en.wikipedia.org/wiki/Approximate_Bayesian_computation\n",
    "\n",
    "# In particolare quello che si fa è:\n",
    "# 1. Sampling dalla prior\n",
    "# 2. Generazione dell'output D_hat dal modello ottenuto con i parametri theta_hat campionati in 1.\n",
    "# 3. Calcolo di distance(D_hat, D), dove D è l'output ottenuto dai dati veri\n",
    "# 4. Se distance(D_hat, D) < epsilon tengo theta_hat, altrimenti butto via e rifaccio\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "# We now start the Bayesian framework\n",
    "# We will compute the Approximate Bayesian Computation approximation from the posterior\n",
    "model.trainable_weights[0].assign(parameters_pre)\n",
    "initial_loss, _ = compute_gradients_and_update(batch_y0, batch_yN)   # The result we obtained from the simulation of the model\n",
    "initial_loss = 0\n",
    "# In teoria questa loss è un minimo locale, quindi dovrebbe essere la più piccola. Più si è vicini a questa\n",
    "# loss con i theta individuati e meglio è per la simulazione.\n",
    "# In alternativa alla loss, per la quale non riesco a trovare giustificazioni teoriche assolutamente convincenti\n",
    "# si possono prendere le traiettorie e calcolare lo scostamento, questo sarebbe un approccio \"esatto\", ma computazionalmente\n",
    "# sbatti...\n",
    "# initial_trajectories = ...\n",
    "niters = 50000\n",
    "naccepted = 0\n",
    "accepted = False\n",
    "eps = 0.1 # To be determined, it is a hyperparameter\n",
    "parameters = np.zeros((niters, para_num))  # book keeping the parameters\n",
    "lambdalist = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "loss = []\n",
    "\n",
    "#lambda_sim = np.exp(npr.normal())\n",
    "lambda_sim = 100\n",
    "w_sim = 0\n",
    "\n",
    "for i in tqdm(range(niters)):\n",
    "\n",
    "    # We will consider here the standard Lasso model for data approximation\n",
    "    #lambda_temp = np.exp(npr.gamma(para_num+1, 1))\n",
    "    \n",
    "    #lambda_temp = npr.randint(low = 10, high = 100)\n",
    "    lambda_temp = npr.uniform(low = 0, high = 10)\n",
    "    \n",
    "    #WW = model.trainable_weights[0].numpy() # illegale\n",
    "    \n",
    "    #npr.laplace(0, scale=1/lambda_sim, size=WW.size),(2,1)\n",
    "    \n",
    "    w_temp = np.resize([npr.normal(0.3, 1/lambda_temp), npr.normal(0.1, 1/lambda_temp)], (2,1))\n",
    "    temp_model.trainable_weights[0].assign(w_temp) \n",
    "\n",
    "    # After the sampling from the prior we go on by simulating the model\n",
    "    # sim_trajectories = simulate_trajectories_from_theta_hat(model, w_temp)\n",
    "    \n",
    "    sim_loss = compute_loss(batch_y0, batch_yN, neural_ode_temp) # ma qua va guardato modello nuovo senza aggiornare\n",
    "    loss.append(sim_loss)\n",
    "\n",
    "    # if abc_distance(initial_trajectories, sim_trajectories) < eps:\n",
    "    if np.abs(initial_loss - sim_loss) < eps:\n",
    "        parameters[i:i+1, :] = np.transpose(w_temp)\n",
    "        lambdalist[i:i+1] = lambda_temp\n",
    "        lambda_sim = lambda_temp\n",
    "        w_sim = w_temp\n",
    "        naccepted += 1\n",
    "        accepted = True\n",
    "\n",
    "    else:\n",
    "        parameters[i:i + 1, :] = np.transpose(w_sim)\n",
    "        lambdalist[i:i + 1] = lambda_sim\n",
    "        accepted = False\n",
    "\n",
    "    #print(\"Accepted = \", accepted)\n",
    "\n",
    "print('Acceptance rate: ', naccepted / niters)\n",
    "\n",
    "np.save('parameters', parameters)  # The Monte Carlo chain of the parameters\n",
    "np.save('lambda', lambdalist)\n",
    "\n",
    "np.savetxt(\"data_weights.csv\", parameters, delimiter=',')\n",
    "np.savetxt(\"data_lambda.csv\", lambdalist, delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_gaussian_sampling(true_center, loc, scale, quantile_1, quantile_2):\n",
    "    if ((sps.norm.cdf(true_center, loc=loc, scale=scale) < quantile_1) | (sps.norm.cdf(true_center, loc=loc, scale=scale) > quantile_2)):\n",
    "        par = npr.uniform(0,1)\n",
    "        a = 0\n",
    "        b = 0\n",
    "        if true_center > loc:\n",
    "            a = ((loc + par*(true_center-loc)) - loc)/scale\n",
    "            b = float('inf')\n",
    "            w_temp = sps.truncnorm.rvs(a, b, loc=loc, scale=scale)\n",
    "        else:\n",
    "            a = ((loc + par*(true_center-loc)) - loc)/scale\n",
    "            b = -float('inf')\n",
    "            w_temp = sps.truncnorm.rvs(b, a, loc=loc, scale=scale)\n",
    "    else:\n",
    "        w_temp = npr.normal(loc, scale)\n",
    "    \n",
    "    return(w_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "\n",
    "# DA QUI IN POI NUOVO\n",
    "\n",
    "# Versione 2.0 - Adaptive Gaussian Sampling\n",
    "\n",
    "# Qui sotto parte una bozza di implementazione del metodo ABC come descritto su\n",
    "# https://en.wikipedia.org/wiki/Approximate_Bayesian_computation\n",
    "\n",
    "# In particolare quello che si fa è:\n",
    "# 1. Sampling dalla prior\n",
    "# 2. Generazione dell'output D_hat dal modello ottenuto con i parametri theta_hat campionati in 1.\n",
    "# 3. Calcolo di distance(D_hat, D), dove D è l'output ottenuto dai dati veri\n",
    "# 4. Se distance(D_hat, D) < epsilon tengo theta_hat, altrimenti butto via e rifaccio\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "import scipy.stats as sps\n",
    "\n",
    "# We now start the Bayesian framework\n",
    "# We will compute the Approximate Bayesian Computation approximation from the posterior\n",
    "model.trainable_weights[0].assign(parameters_pre)\n",
    "initial_loss, _ = compute_gradients_and_update(batch_y0, batch_yN)   # The result we obtained from the simulation of the model\n",
    "initial_loss = 0\n",
    "# In teoria questa loss è un minimo locale, quindi dovrebbe essere la più piccola. Più si è vicini a questa\n",
    "# loss con i theta individuati e meglio è per la simulazione.\n",
    "# In alternativa alla loss, per la quale non riesco a trovare giustificazioni teoriche assolutamente convincenti\n",
    "# si possono prendere le traiettorie e calcolare lo scostamento, questo sarebbe un approccio \"esatto\", ma computazionalmente\n",
    "# sbatti...\n",
    "# initial_trajectories = ...\n",
    "niters = 50000\n",
    "naccepted = 0\n",
    "eps = 0.1 # To be determined, it is a hyperparameter\n",
    "parameters = np.zeros((niters, para_num))  # book keeping the parameters\n",
    "lambdalist = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "loss = []\n",
    "\n",
    "lambda_temp = 0\n",
    "w_temp = 0\n",
    "\n",
    "for i in tqdm(range(niters)):\n",
    "\n",
    "    # We will consider here the standard Ridge model for data approximation\n",
    "\n",
    "    lambda_temp = npr.uniform(low = 0, high = 10)\n",
    "    \n",
    "    WW = model.trainable_weights[0].numpy()\n",
    "    \n",
    "    w_temp_1 = 0\n",
    "    w_temp_2 = 0\n",
    "    \n",
    "    alpha = 0.15\n",
    "    \n",
    "    w_temp_1 = adaptive_gaussian_sampling(WW[0], 0, 1/lambda_temp, alpha, 1-alpha)\n",
    "    w_temp_2 = adaptive_gaussian_sampling(WW[1], 0, 1/lambda_temp, alpha, 1-alpha)\n",
    "    \n",
    "    w_temp = np.resize([w_temp_1, w_temp_2], (2,1))\n",
    "    temp_model.trainable_weights[0].assign(w_temp) \n",
    "\n",
    "    # After the sampling from the prior we go on by simulating the model\n",
    "    # sim_trajectories = simulate_trajectories_from_theta_hat(model, w_temp)\n",
    "    \n",
    "    sim_loss = compute_loss(batch_y0, batch_yN, neural_ode_temp) # ma qua va guardato modello nuovo senza aggiornare\n",
    "    loss.append(sim_loss)\n",
    "\n",
    "    # if abc_distance(initial_trajectories, sim_trajectories) < eps:\n",
    "    if np.abs(initial_loss - sim_loss) < (eps + npr.uniform()*eps*1/3):\n",
    "        parameters[naccepted:naccepted+1, :] = np.transpose(w_temp)\n",
    "        lambdalist[naccepted:naccepted+1] = lambda_temp\n",
    "        naccepted += 1\n",
    "\n",
    "print('Acceptance rate: ', naccepted / niters)\n",
    "\n",
    "parameters = parameters[0:naccepted,:]\n",
    "lambdalist = lambdalist[0:naccepted,:]\n",
    "\n",
    "np.save('parameters', parameters)  # The Monte Carlo chain of the parameters\n",
    "np.save('lambda', lambdalist)\n",
    "\n",
    "np.savetxt(\"data_weights.csv\", parameters, delimiter=',')\n",
    "np.savetxt(\"data_lambda.csv\", lambdalist, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "border1 = np.min(parameters[:,0])\n",
    "border2 = np.min(parameters[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_abc_gaussian_sampling(true_center, loc, scale, border):\n",
    "    if ((sps.norm.cdf(true_center, loc=loc, scale=scale) < 0.15) | (sps.norm.cdf(true_center, loc=loc, scale=scale) > 0.85)):\n",
    "        a = 0\n",
    "        b = 0\n",
    "        if true_center > loc:\n",
    "            a = (border - loc)/scale\n",
    "            b = float('inf')\n",
    "            w_temp = sps.truncnorm.rvs(a, b, loc=loc, scale=scale)\n",
    "        else:\n",
    "            a = (border - loc)/scale\n",
    "            b = -float('inf')\n",
    "            w_temp = sps.truncnorm.rvs(b, a, loc=loc, scale=scale)\n",
    "    else:\n",
    "        w_temp = npr.normal(loc, scale)\n",
    "    \n",
    "    return(w_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "\n",
    "# DA QUI IN POI NUOVO\n",
    "\n",
    "# Versione 3.0 - Adaptive Gaussian Sampling\n",
    "\n",
    "# Qui sotto parte una bozza di implementazione del metodo ABC come descritto su\n",
    "# https://en.wikipedia.org/wiki/Approximate_Bayesian_computation\n",
    "\n",
    "# In particolare quello che si fa è:\n",
    "# 1. Sampling dalla prior\n",
    "# 2. Generazione dell'output D_hat dal modello ottenuto con i parametri theta_hat campionati in 1.\n",
    "# 3. Calcolo di distance(D_hat, D), dove D è l'output ottenuto dai dati veri\n",
    "# 4. Se distance(D_hat, D) < epsilon tengo theta_hat, altrimenti butto via e rifaccio\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "import scipy.stats as sps\n",
    "\n",
    "# We now start the Bayesian framework\n",
    "# We will compute the Approximate Bayesian Computation approximation from the posterior\n",
    "model.trainable_weights[0].assign(parameters_pre)\n",
    "initial_loss, _ = compute_gradients_and_update(batch_y0, batch_yN)   # The result we obtained from the simulation of the model\n",
    "initial_loss = 0\n",
    "# In teoria questa loss è un minimo locale, quindi dovrebbe essere la più piccola. Più si è vicini a questa\n",
    "# loss con i theta individuati e meglio è per la simulazione.\n",
    "# In alternativa alla loss, per la quale non riesco a trovare giustificazioni teoriche assolutamente convincenti\n",
    "# si possono prendere le traiettorie e calcolare lo scostamento, questo sarebbe un approccio \"esatto\", ma computazionalmente\n",
    "# sbatti...\n",
    "# initial_trajectories = ...\n",
    "niters = 50000\n",
    "naccepted = 0\n",
    "eps = 0.1 # To be determined, it is a hyperparameter\n",
    "parameters = np.zeros((niters, para_num))  # book keeping the parameters\n",
    "lambdalist = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "loss = []\n",
    "\n",
    "lambda_temp = 0\n",
    "w_temp = 0\n",
    "\n",
    "for i in tqdm(range(niters)):\n",
    "\n",
    "    # We will consider here the standard Ridge model for data approximation\n",
    "\n",
    "    lambda_temp = npr.uniform(low = 0, high = 10)\n",
    "    \n",
    "    WW = model.trainable_weights[0].numpy()\n",
    "    \n",
    "    w_temp_1 = 0\n",
    "    w_temp_2 = 0\n",
    "    \n",
    "    alpha = 0.15\n",
    "    \n",
    "    w_temp_1 = e_abc_gaussian_sampling(WW[0], 0, 1/lambda_temp, border1)\n",
    "    w_temp_2 = e_abc_gaussian_sampling(WW[1], 0, 1/lambda_temp, border2)\n",
    "    \n",
    "    w_temp = np.resize([w_temp_1, w_temp_2], (2,1))\n",
    "    temp_model.trainable_weights[0].assign(w_temp) \n",
    "\n",
    "    # After the sampling from the prior we go on by simulating the model\n",
    "    # sim_trajectories = simulate_trajectories_from_theta_hat(model, w_temp)\n",
    "    \n",
    "    sim_loss = compute_loss(batch_y0, batch_yN, neural_ode_temp) # ma qua va guardato modello nuovo senza aggiornare\n",
    "    loss.append(sim_loss)\n",
    "\n",
    "    # if abc_distance(initial_trajectories, sim_trajectories) < eps:\n",
    "    if np.abs(initial_loss - sim_loss) < (eps + npr.uniform()*eps*1/3):\n",
    "        parameters[naccepted:naccepted+1, :] = np.transpose(w_temp)\n",
    "        lambdalist[naccepted:naccepted+1] = lambda_temp\n",
    "        naccepted += 1\n",
    "\n",
    "print('Acceptance rate: ', naccepted / niters)\n",
    "\n",
    "parameters = parameters[0:naccepted,:]\n",
    "lambdalist = lambdalist[0:naccepted,:]\n",
    "\n",
    "np.save('parameters', parameters)  # The Monte Carlo chain of the parameters\n",
    "np.save('lambda', lambdalist)\n",
    "\n",
    "np.savetxt(\"data_weights.csv\", parameters, delimiter=',')\n",
    "np.savetxt(\"data_lambda.csv\", lambdalist, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1 - 2\n",
    "b = float('inf')\n",
    "\n",
    "print(sps.truncnorm.rvs(a, b, loc=2, scale=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambdalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "g = sns.PairGrid(pd.DataFrame(np.unique(parameters, axis = 0)))\n",
    "g.map_upper(sns.scatterplot, s = 5)\n",
    "g.map_lower(sns.kdeplot, fill=True)\n",
    "g.map_diag(sns.histplot, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_grid = np.linspace(0, data_size, data_size)  # uniformly spaced data? -> even though advantage is learning with not uniformly spaced data\n",
    "z0 = [N - infected_0, infected_0, 0] # initial conditions\n",
    "\n",
    "for j in tqdm(range(parameters.shape[0])):\n",
    "    simul_yy =  odeint(SIR, z0, t_grid, args=tuple(parameters[j]))\n",
    "    plot_traj_gray(simul_yy)\n",
    "\n",
    "true_yy = odeint(SIR, z0, t_grid, args=(beta, gamma))  # potrebbe aver senso tenerli in memoria se è lento\n",
    "plot_traj(true_yy)\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.ess(parameters[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(parameters[:,0],hdi_prob = 0.95, ref_val = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(parameters[:,1],hdi_prob = 0.95, ref_val = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_mcse(parameters[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "\n",
    "# - simulation with only two parameters\n",
    "# - go back to noisy simulation\n",
    "# - thinning\n",
    "# - weighted noise\n",
    "# - CI for intensive therapy\n",
    "# - sensitivity analysis!\n",
    "\n",
    "# - sliding window for parameters estimation?\n",
    "# - non observable states' estimation (through t-1?)\n",
    "# - different dynamical systems interactions \n",
    "\n",
    "# - batch does not really make sense\n",
    "# -> we always overestimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
