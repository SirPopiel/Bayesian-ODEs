{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "#keras = tf.keras\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# from neural_ode import NeuralODE\n",
    "\n",
    "import sys\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "target_dir = os.path.sep.join(current_dir.split(os.path.sep)[:])\n",
    "sys.path.append(os.path.join(os.path.dirname(target_dir),'../eABCSMC'))\n",
    "sys.path.append(os.path.join(os.path.dirname(target_dir),'../Epidemics_response/Data'))\n",
    "sys.path.append(target_dir)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIRD_reduced_30_new(z, t, beta0, gamma, mu0, alpha):\n",
    "\n",
    "    I, R, D = z\n",
    "    \n",
    "    S0 = 3.50860049e+01 * 100000 * beta0/10\n",
    "    S = S0 - I - R - D\n",
    "    mu = mu0/1000\n",
    "    beta = beta0/10 * np.exp(-(alpha/1000) * t)\n",
    "    dS = - beta * I * S / S0\n",
    "    dI = beta * I * S / S0 - gamma/100 * I - mu * I\n",
    "    dR = gamma/100 * I\n",
    "    dD = mu * I\n",
    "\n",
    "    dzdt = [dI, dR, dD]\n",
    "    return dzdt\n",
    "\n",
    "def SIRD_reduced_30_new_ER(z, t, beta0, gamma, mu0, alpha):\n",
    "\n",
    "    I, R, D = z\n",
    "    \n",
    "    S0 = 4.73028175e+01 * 100000 * beta0/10\n",
    "    S = S0 - I - R - D\n",
    "    mu = mu0/1000\n",
    "    beta = beta0/10 * np.exp(-(alpha/1000) * t)\n",
    "    dS = - beta * I * S / S0\n",
    "    dI = beta * I * S / S0 - gamma/100 * I - mu * I\n",
    "    dR = gamma/100 * I\n",
    "    dD = mu * I\n",
    "\n",
    "    dzdt = [dI, dR, dD]\n",
    "    return dzdt\n",
    "\n",
    "\n",
    "def plot_traj_SIRD(trajectories, width = 1.):\n",
    "    x2 = trajectories[:,0]\n",
    "    x3 = trajectories[:,1]\n",
    "    x4 = trajectories[:,2]\n",
    "\n",
    "    i = plt.plot(x2, linewidth = width, label = 'Infected')\n",
    "    r = plt.plot(x3, linewidth = width, label = 'Recovered')\n",
    "    d = plt.plot(x4, linewidth = width, label = 'Deceased')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    #plt.title('Real SIRD')\n",
    "    plt.title('nCov-19 data, Italy')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sir.png', dpi=500)\n",
    "    \n",
    "    \n",
    "def plot_traj_gray_SIRD(trajectories, width = 1.):\n",
    "    x2 = trajectories[:,0]\n",
    "    x3 = trajectories[:,1]\n",
    "    x4 = trajectories[:,2]\n",
    "    i = plt.plot(x2, linewidth = width, color = 'lightgray')\n",
    "    r = plt.plot(x3, linewidth = width, color = 'lightgray')\n",
    "    d = plt.plot(x4, linewidth = width, color = 'lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Region = 'Lombardia'\n",
    "\n",
    "flag_r = (Region=='Veneto')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if flag_r:\n",
    "    italy = pd.read_csv('../../Data/RegionLombardia.csv')\n",
    "else:\n",
    "    italy = pd.read_csv('../../Data/RegionVeneto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = 53+10\n",
    "eff = 53+10\n",
    "start = 260\n",
    "SIRD_flag = True\n",
    "italy = italy[start:]\n",
    "italy = italy[:ext]\n",
    "italy.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = italy.shape[0]\n",
    "\n",
    "batch_time = 20 # tempo delle batches\n",
    "niters = 500\n",
    "batch_size = data_size - batch_time - 1\n",
    "\n",
    "if SIRD_flag:\n",
    "    extended_y = []\n",
    "    for i in range(ext):\n",
    "        extended_y.append([italy.loc[i,'I'],italy.loc[i,'R'],italy.loc[i,'D']])\n",
    "    extended_y = np.array(extended_y)\n",
    "else:\n",
    "    extended_y = []\n",
    "    for i in range(ext):\n",
    "        extended_y.append([italy.loc[i,'I'],italy.loc[i,'R']+italy.loc[i,'D']])\n",
    "    extended_y = np.array(extended_y)\n",
    "\n",
    "nonorm_y = extended_y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy = italy[:eff]\n",
    "italy.reset_index(inplace = True, drop = True)\n",
    "#italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = italy.shape[0]\n",
    "\n",
    "batch_time = 15  # tempo delle batches\n",
    "niters = 500\n",
    "batch_size = data_size - batch_time - 1\n",
    "\n",
    "if SIRD_flag:\n",
    "    true_y = []\n",
    "    for i in range(data_size):\n",
    "        true_y.append([italy.loc[i,'I'],italy.loc[i,'R'],italy.loc[i,'D']])\n",
    "    true_y = np.array(true_y)\n",
    "else:\n",
    "    true_y = []\n",
    "    for i in range(data_size):\n",
    "        true_y.append([italy.loc[i,'I'],italy.loc[i,'R']+italy.loc[i,'D']])\n",
    "    true_y = np.array(true_y)\n",
    "\n",
    "nonorm_y = true_y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(7, 4), dpi=150, facecolor='w', edgecolor='k')\n",
    "\n",
    "plot_traj_SIRD(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_start = np.float32(np.array(italy.iloc[0,0:4]))\n",
    "print(china_start)\n",
    "\n",
    "if flag_r:\n",
    "    italy_start = tuple(china_start)\n",
    "else:\n",
    "    italy_start = tuple(china_start)\n",
    "    \n",
    "italy_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = italy.shape[0]\n",
    "t_grid = np.arange(data_size)\n",
    "\n",
    "t_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate, optimize\n",
    "\n",
    "true_yy = np.append(true_y[:,0],true_y[:,1])\n",
    "true_yy = np.append(true_yy,true_y[:,2])\n",
    "    \n",
    "def fit_odeint_reduced_30_new(x, beta0, gamma, mu0, alpha):\n",
    "    fit = integrate.odeint(SIRD_reduced_30_new, italy_start, x, args=(beta0, gamma, mu0, alpha))\n",
    "    fit_p = np.append(fit[:,0],fit[:,1])\n",
    "    return np.append(fit_p,fit[:,2])\n",
    "\n",
    "def fit_odeint_reduced_30_new_ER(x, beta0, gamma, mu0, alpha):\n",
    "    fit = integrate.odeint(SIRD_reduced_30_new_ER, italy_start, x, args=(beta0, gamma, mu0, alpha))\n",
    "    fit_p = np.append(fit[:,0],fit[:,1])\n",
    "    return np.append(fit_p,fit[:,2])\n",
    "\n",
    "if flag_r:\n",
    "    popt, pcov = optimize.curve_fit(fit_odeint_reduced_30_new, xdata = t_grid, ydata =  true_yy, p0 = (0.43828327*10,  0.02015063*100,  0.30317892*10, 5), bounds=([0, 0, 0, -10], [10, 7, 5, 10]), method='trf')\n",
    "else:\n",
    "    popt, pcov = optimize.curve_fit(fit_odeint_reduced_30_new_ER, xdata = t_grid, ydata =  true_yy, p0 = (0.43828327*10,  0.02015063*100, 0.30317892*10, -5), bounds=([0, 0, 0, -30], [500, 7, 5, 100]), method='trf')\n",
    "\n",
    "    \n",
    "popt, pcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_r:\n",
    "    simul_yy =  odeint(SIRD_reduced_30_new, italy_start, np.arange(ext), args=tuple(np.reshape(popt,(1,-1))[0]))\n",
    "    plot_traj_SIRD(simul_yy)\n",
    "    plot_traj_gray_SIRD(extended_y)\n",
    "    \n",
    "else:\n",
    "    simul_yy =  odeint(SIRD_reduced_30_new_ER, italy_start, np.arange(ext), args=tuple(np.reshape(popt,(1,-1))[0]))\n",
    "    plot_traj_SIRD(simul_yy)\n",
    "    plot_traj_gray_SIRD(extended_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_yy = np.append(extended_y[:,0],extended_y[:,1])\n",
    "extended_yy = np.append(extended_yy,extended_y[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.ravel(extended_yy) - fit_odeint_reduced_30_new_ER(np.arange(ext), *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "\n",
    "ss_tot = np.sum((np.ravel(extended_yy)-np.ravel(np.mean(extended_yy)))**2)\n",
    "\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "r_squared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr(covariance):\n",
    "    v = np.sqrt(np.diag(covariance))\n",
    "    outer_v = np.outer(v, v)\n",
    "    correlation = covariance / outer_v\n",
    "    correlation[covariance == 0] = 0\n",
    "    return correlation\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "corr = compute_corr(pcov)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "sns.heatmap(corr, annot = True)\n",
    "\n",
    "plt.title(\"Correlation Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_param = 4 # Number of parameters\n",
    "para_num = num_param\n",
    "\n",
    "t0 = t_grid[:batch_time][0]  # t0 = first element of t_grid\n",
    "t1 = t_grid[:batch_time][-1]  # t1 = the element of t_grid at batch_time\n",
    "t_in = np.linspace(t0, t1, 10)  # The time grid between t0 and t1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(params):\n",
    "    residuals = true_yy - fit_odeint_reduced_30_new(np.arange(eff), *params)\n",
    "    loss = np.sum(residuals**2)/(1e9)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 2 functions sample from a truncated Gamma distribution by using the inverse cdf method. The first one\n",
    "#  samples from the interval [a, +inf), the second one from [0, a)\n",
    "\n",
    "import scipy.stats as sps\n",
    "import numpy.random as npr\n",
    "\n",
    "def trGamma_a_inf(shape, rate, trunc):\n",
    "    interval = 1 - sps.gamma.cdf(trunc, a = shape, scale = 1/rate)\n",
    "    yr = npr.rand(1)*interval + sps.gamma.cdf(trunc, a = shape, scale = 1/rate)\n",
    "    xr = sps.gamma.ppf(yr, a = shape, scale = 1/rate)\n",
    "    return(xr[0])\n",
    "\n",
    "def trGamma_0_a(shape, rate, trunc):\n",
    "    interval = sps.gamma.cdf(trunc, a = shape, scale = 1/rate)\n",
    "    yr = npr.rand(1)*interval\n",
    "    xr = sps.gamma.ppf(yr, a = shape, scale = 1/rate)\n",
    "    return(xr[0])\n",
    "\n",
    "def adaptive_gaussian_sampling(true_center, loc, scale, quantile_1, quantile_2):\n",
    "    if ((sps.norm.cdf(true_center, loc=loc, scale=scale) < quantile_1) | (sps.norm.cdf(true_center, loc=loc, scale=scale) > quantile_2)):\n",
    "        par = npr.uniform(0,1)\n",
    "        a = 0\n",
    "        b = 0\n",
    "        if true_center > loc:\n",
    "            a = ((loc + par*(true_center-loc)) - loc)/scale\n",
    "            b = float('inf')\n",
    "            w_temp = sps.truncnorm.rvs(a, b, loc=loc, scale=scale)\n",
    "        else:\n",
    "            a = ((loc + par*(true_center-loc)) - loc)/scale\n",
    "            b = -float('inf')\n",
    "            w_temp = sps.truncnorm.rvs(b, a, loc=loc, scale=scale)\n",
    "    else:\n",
    "        w_temp = npr.normal(loc, scale)\n",
    "    \n",
    "    return(w_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_gamma_sampling(true_center, shape, rate, quantile_1, quantile_2):\n",
    "    if ((sps.gamma.cdf(true_center, a=shape, scale=1/rate) < quantile_1) | (sps.gamma.cdf(true_center, a=shape, scale=1/rate) > quantile_2)):\n",
    "        par = npr.uniform(0,1)\n",
    "        a = 0\n",
    "        b = 0\n",
    "        loc = sps.gamma.mean(a=shape, scale=1/rate)\n",
    "        if true_center > loc:\n",
    "            trunc = loc + par*(true_center-loc)\n",
    "            w_temp = trGamma_a_inf(shape=shape, rate=rate, trunc=trunc)\n",
    "        else:\n",
    "            trunc = loc + par*(true_center-loc)\n",
    "            w_temp = trGamma_0_a(shape=shape, rate=rate, trunc=trunc)\n",
    "    else:\n",
    "        w_temp = sps.gamma.rvs(a = shape, scale=1/rate)\n",
    "    \n",
    "    return(w_temp)\n",
    "\n",
    "\n",
    "def border_estimates_e_abc(eps, niters):\n",
    "    \n",
    "    \n",
    "    # This function estimates the borders of the region containing the eps-approximate posterior through niters \n",
    "    #  iterations. It uses the empirical sampling method to get nearer to the correct acceptance region.\n",
    "    #  This is only used to estimate borders, though it can estimate also the parameters in order to avoid \n",
    "    #  biases in the estimation.\n",
    "\n",
    "    import scipy.stats as sps\n",
    "\n",
    "    initial_loss = 0\n",
    "    \n",
    "    naccepted = 0\n",
    "    \n",
    "    parameters = np.zeros((niters, para_num))  # book keeping the parameters\n",
    "    lambdalist = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "    loss = []\n",
    "\n",
    "    lambda_temp = 0\n",
    "    w_temp = np.zeros(num_param)\n",
    "    \n",
    "    prior_means = np.array([4.3828327, 2.015063, 9.293047 , 10])  # Estimations from Italy (Adjusted for alpha because of hard lockdown)\n",
    "    \n",
    "    for i in tqdm(range(niters)):\n",
    "\n",
    "        lambda_temp = [npr.uniform(low = 0.5, high = 10),npr.uniform(low = 0.5, high = 10),npr.uniform(low = 0.5, high = 10),npr.uniform(low = 0.5, high = 5)] #200\n",
    "\n",
    "        alpha_quant = 0.25\n",
    "\n",
    "        # Sampling of the candidate point from the priors\n",
    "        \n",
    "        w_temp[0] = adaptive_gamma_sampling(true_center=popt[0], shape=lambda_temp[0]*prior_means[0], rate=lambda_temp[0], quantile_1=alpha_quant, quantile_2=1-alpha_quant)\n",
    "        w_temp[1] = adaptive_gamma_sampling(true_center=popt[1], shape=lambda_temp[1]*prior_means[1], rate=lambda_temp[1], quantile_1=alpha_quant, quantile_2=1-alpha_quant)\n",
    "        w_temp[2] = adaptive_gamma_sampling(true_center=popt[2], shape=lambda_temp[2]*prior_means[2], rate=lambda_temp[2], quantile_1=alpha_quant, quantile_2=1-alpha_quant)\n",
    "        w_temp[3] = adaptive_gaussian_sampling(true_center=popt[3], loc=0, scale=lambda_temp[3], quantile_1=alpha_quant, quantile_2=1-alpha_quant)      \n",
    "        \n",
    "        # After the sampling from the prior we go on by simulating the model\n",
    "        # sim_trajectories = simulate_trajectories_from_theta_hat(model, w_temp)\n",
    "\n",
    "        sim_loss = compute_loss(w_temp)\n",
    "        loss.append(sim_loss)\n",
    "        \n",
    "        # Acceptance condition\n",
    "\n",
    "        if np.abs(initial_loss - sim_loss) < (eps + npr.uniform()*eps*1/3) and all(w_temp[pos] >= 0 for pos in range(3)):  \n",
    "            parameters[naccepted:naccepted+1, :] = np.transpose(w_temp)\n",
    "            naccepted += 1\n",
    "\n",
    "    print('Acceptance rate: ', naccepted / niters)\n",
    "\n",
    "    parameters = parameters[0:naccepted,:]\n",
    "    \n",
    "    borders = [np.min(parameters[:,i]) for i in range(num_param-1)]\n",
    "    borders.append(np.max(parameters[:,(num_param-1)]))\n",
    "\n",
    "    return(borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_abc_gamma_sampling(true_center, shape, rate, border):\n",
    "    \n",
    "    # This function performs the empirical sampling from our method when the \"true center\" is too far from\n",
    "    #  the center of the prior.\n",
    "    \n",
    "    loc = sps.gamma.mean(a=shape, scale=1/rate)\n",
    "    q = 0.25\n",
    "    \n",
    "    w_temp = trGamma_a_inf(shape=shape, rate=rate, trunc=border)\n",
    "    \n",
    "    return(w_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_abc_gaussian_sampling(true_center, loc, scale, border):\n",
    "    \n",
    "    # This function performs the empirical sampling from our method when the \"true center\" is too far from\n",
    "    #  the center of the prior.\n",
    "    \n",
    "    if ((sps.norm.cdf(true_center, loc=loc, scale=scale) < 0.25) | (sps.norm.cdf(true_center, loc=loc, scale=scale) > 0.75)):\n",
    "        a = 0\n",
    "        b = 0\n",
    "        if (true_center > loc):\n",
    "            a = (border - loc)/scale\n",
    "            b = float('inf')\n",
    "            w_temp = sps.truncnorm.rvs(a, b, loc=loc, scale=scale)\n",
    "        else:\n",
    "            a = (border - loc)/scale\n",
    "            b = -float('inf')\n",
    "            w_temp = sps.truncnorm.rvs(b, a, loc=loc, scale=scale)\n",
    "    else:\n",
    "        w_temp = npr.normal(loc, scale)\n",
    "    \n",
    "    return(w_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_e_abc(eps, niters, borders):\n",
    "\n",
    "    # This function performs the first sampling from the region defined through the borders estimating function and\n",
    "    #  returns the estimated parameters and the initializing weights, ([1,1,1,...,1] normalized)\n",
    "    \n",
    "    import scipy.stats as sps\n",
    "    initial_loss = 0\n",
    "\n",
    "    naccepted = 0\n",
    "\n",
    "    parameters = np.zeros((niters, para_num))  # book keeping the parameters\n",
    "    lambdalist = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "    loss = []\n",
    "\n",
    "    lambda_temp = 0\n",
    "    w_temp = np.zeros(para_num)\n",
    "\n",
    "    prior_means = np.array([4.3828327, 2.015063, 9.293047 , 10])  # Estimations from Italy (Adjusted for alpha because of hard lockdown)\n",
    "    \n",
    "    for i in tqdm(range(niters)):\n",
    "\n",
    "        lambda_temp = [npr.uniform(low = 0.5, high = 5),npr.uniform(low = 0.5, high = 5),npr.uniform(low = 0.5, high = 5),npr.uniform(low = 0.5, high = 7)] \n",
    "\n",
    "        #w_temp = [e_abc_gamma_sampling(popt[j], lambda_temp[j]*prior_means[j], lambda_temp[j], borders[j]) for j in range(len(popt))]\n",
    "        \n",
    "        w_temp[0] = e_abc_gamma_sampling(true_center=popt[0], shape=lambda_temp[0]*prior_means[0], rate=lambda_temp[0], border=borders[0])\n",
    "        w_temp[1] = e_abc_gamma_sampling(true_center=popt[1], shape=lambda_temp[1]*prior_means[1], rate=lambda_temp[1], border=borders[1])\n",
    "        w_temp[2] = e_abc_gamma_sampling(true_center=popt[2], shape=lambda_temp[2]*prior_means[2], rate=lambda_temp[2], border=borders[2])\n",
    "        w_temp[3] = e_abc_gaussian_sampling(true_center=popt[3], loc=0, scale=lambda_temp[3], border=borders[3])      \n",
    "        \n",
    "        sim_loss =  compute_loss(w_temp)\n",
    "        loss.append(sim_loss)\n",
    "\n",
    "        #if np.abs(initial_loss - sim_loss) < (eps + npr.uniform()*eps*1/3) and (w_temp[3] >= 2*w_temp[2]):\n",
    "        if np.abs(initial_loss - sim_loss) < (eps + npr.uniform()*eps*1/3):\n",
    "            parameters[naccepted:naccepted+1, :] = np.transpose(w_temp)\n",
    "            #lambdalist[naccepted:naccepted+1] = lambda_temp\n",
    "            naccepted += 1\n",
    "\n",
    "    print('Acceptance rate: ', naccepted / niters)\n",
    "\n",
    "    parameters = parameters[0:naccepted,:]\n",
    "    #lambdalist = lambdalist[0:naccepted,:]\n",
    "\n",
    "    weights = np.ones(parameters.shape[0])/parameters.shape[0]\n",
    "    \n",
    "    return(parameters, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights_abc_smc(w, loc, scale, prev_w, prev_p, scale_kernel):\n",
    "    \n",
    "    # This function computes the weights associated to each parameter as described in https://arxiv.org/pdf/1106.6280.pdf\n",
    "    \n",
    "    prob_w = 1\n",
    "    for i in range(para_num):\n",
    "        prob_w *= sps.norm.pdf(w[i], loc=loc, scale=scale)\n",
    "    \n",
    "    previous_w = 0\n",
    "    for i in range(prev_w.shape[0]):\n",
    "        kern_w = 1\n",
    "        for j in range(para_num):\n",
    "            kern_w *= sps.norm.pdf(w[j], loc=prev_p[i,j], scale=scale_kernel[j])\n",
    "        previous_w += prev_w[i]*kern_w\n",
    "    \n",
    "    return(prob_w/previous_w)\n",
    "\n",
    "def normalize_weights(weights):\n",
    "    \n",
    "    # This function performs the normalization of weights\n",
    "    \n",
    "    tot_weight = np.sum(weights)\n",
    "    return(weights/tot_weight)\n",
    "\n",
    "def sample_abc_smc_element(parameters, weights):\n",
    "    \n",
    "    # This function samples from the previous population according to the specified weights\n",
    "    \n",
    "    elements = np.arange(parameters.shape[0])\n",
    "    idx = np.random.choice(elements, 1, p=weights)\n",
    "    return(parameters[idx,])\n",
    "\n",
    "def perturbation_kernel(sdev):\n",
    "    \n",
    "    # This function returns the perturbation from a Gaussian kernel with the specified standard deviation\n",
    "    \n",
    "    return([np.random.randn()*sdev[i] for i in range(para_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_abc_smc(eps, niters, kernel_std, old_parameters, weights): \n",
    "    \n",
    "    # This function returns the sampling according to the ABC-SMC with the weights associated to old parameters\n",
    "    # specified in weights and the old parameters specified in old_parameters\n",
    "    \n",
    "    import scipy.stats as sps\n",
    "\n",
    "    initial_loss = 0\n",
    "\n",
    "    naccepted = 0\n",
    "    parameters = np.zeros((niters, para_num))  # book keeping the parameters\n",
    "    lambdalist = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "    loss = []\n",
    "    new_weights = []\n",
    "\n",
    "    lambda_temp = 0\n",
    "    w_temp = np.zeros(para_num)\n",
    "\n",
    "    for i in tqdm(range(niters)):\n",
    "\n",
    "        # Extracting from previous population with specified weights\n",
    "        w_temp = sample_abc_smc_element(old_parameters, weights)\n",
    "        \n",
    "        # Perturbating with the gaussian Kernel\n",
    "        pert = perturbation_kernel(kernel_std)\n",
    "        w_temp = w_temp + pert\n",
    "\n",
    "        w_temp = np.resize(w_temp, (para_num,))\n",
    "\n",
    "        sim_loss =  compute_loss(w_temp)\n",
    "        loss.append(sim_loss)\n",
    "\n",
    "        #if (np.abs(initial_loss - sim_loss) < (eps + npr.uniform()*eps*1/3)) and (min(w_temp) >= 0) and (w_temp[3] >= 2*w_temp[2]):\n",
    "        if (np.abs(initial_loss - sim_loss) < (eps + npr.uniform()*eps*1/3)) and (min(w_temp[0:3]) >= 0):\n",
    "            parameters[naccepted:naccepted+1, :] = np.transpose(w_temp)\n",
    "            #lambdalist[naccepted:naccepted+1] = lambda_temp\n",
    "            naccepted += 1\n",
    "            lambda_temp = npr.uniform(low = 0, high = 1.5) # CAMBIARE LA FUNZIONE CHE PRENDE IN INGRESSO\n",
    "            new_weights.append(compute_weights_abc_smc(w_temp, 0, 1/lambda_temp, weights, old_parameters, kernel_std)) \n",
    "            \n",
    "        \n",
    "    print('Acceptance rate: ', naccepted / niters)\n",
    "\n",
    "    new_weights = normalize_weights(new_weights)\n",
    "    new_weights = new_weights.reshape(naccepted)\n",
    "    \n",
    "    parameters = parameters[0:naccepted,:]\n",
    "    #lambdalist = lambdalist[0:naccepted,:]\n",
    "    \n",
    "    return(parameters, new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sps\n",
    "\n",
    "eps = 10*compute_loss(popt)\n",
    "niters = 3000\n",
    "\n",
    "print(\"Borders estimation start...\")\n",
    "borders = border_estimates_e_abc(eps, 1*niters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(borders)\n",
    "print(\"Borders estimation completed, starting preprocessing...\")\n",
    "start, start_weights = preprocessing_e_abc(eps/7, 50*niters, borders)\n",
    "print(\"Preprocessing completed, starting ABC-SMC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiare i lambda anche in smc\n",
    "\n",
    "niters = 3000\n",
    "\n",
    "parameters, weights = sample_abc_smc(eps/15, 6*niters, np.std(start, axis=0), start, start_weights)\n",
    "#parameters, weights = sample_abc_smc(eps/6, 2*niters, np.std(parameters, axis=0), parameters, weights)\n",
    "#parameters, weights = sample_abc_smc(eps/6, 8*niters, np.std(parameters, axis=0), parameters, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters=start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "g = sns.PairGrid(pd.DataFrame(np.unique(parameters, axis = 0)))\n",
    "g.map_upper(sns.scatterplot, s = 5)\n",
    "g.map_lower(sns.kdeplot, fill=True)\n",
    "g.map_diag(sns.histplot, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.plot_posterior(parameters[:,0],hdi_prob = 0.95)\n",
    "az.plot_posterior(parameters[:,1],hdi_prob = 0.95)\n",
    "az.plot_posterior(parameters[:,2],hdi_prob = 0.95)\n",
    "az.plot_posterior(parameters[:,3],hdi_prob = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('parameters_new.npy',parameters)\n",
    "#np.save('weights_new.npy',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = np.load('parameters_veneto.npy')\n",
    "#weights= np.load('weights_new.npy)\n",
    "ext = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters[1000:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj_SIRD(trajectories, width = 1.9):\n",
    "    x2 = trajectories[:,0]\n",
    "    x3 = trajectories[:,1]\n",
    "    x4 = trajectories[:,2]\n",
    "\n",
    "    i = plt.plot(x2, linewidth=width, label='Infected', color = 'navy')\n",
    "    r = plt.plot(x3, linewidth=width, label='Recovered', color = (.21,.39,.55))\n",
    "    d = plt.plot(x4, linewidth=width, label='Deceased', color = (0.39,.72,1))\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    #plt.title('Real SIRD')\n",
    "    plt.title('nCov-19 data, Veneto')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sir.png', dpi=500)\n",
    "    \n",
    "def plot_traj_SIRD2(trajectories, width=1):\n",
    "    x2 = trajectories[:, 0]\n",
    "    x3 = trajectories[:, 1]\n",
    "    x4 = trajectories[:, 2]\n",
    "    color_ = (0,0,0.5)\n",
    "    \n",
    "\n",
    "    i = plt.plot(x2, linewidth=width,color = 'navy', linestyle = 'dashed' )\n",
    "    r = plt.plot(x3, linewidth=width, color = (.21,.39,.55), linestyle = 'dashed')\n",
    "    d = plt.plot(x4, linewidth=width,  color = (0.39,.72,1), linestyle = 'dashed')\n",
    "\n",
    "    # plt.title('Real SIRD')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (9,6))\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "simul_all = []\n",
    "\n",
    "peak = []\n",
    "day_peak = []\n",
    "\n",
    "trajectories2storeI = np.zeros(shape=(parameters.shape[0], ext))\n",
    "trajectories2storeR = np.zeros(shape=(parameters.shape[0], ext))\n",
    "trajectories2storeD = np.zeros(shape=(parameters.shape[0], ext))\n",
    "\n",
    "for j in tqdm(range(parameters.shape[0])):\n",
    "    simul_yy = odeint(SIRD_reduced_30_new, italy_start, np.arange(ext), args=tuple(parameters[j]))\n",
    "    simul_all.append(simul_yy)\n",
    "    trajectories2storeI[j,:] = simul_yy[:,0]\n",
    "    trajectories2storeR[j,:] = simul_yy[:,1]\n",
    "    trajectories2storeD[j,:] = simul_yy[:,2]\n",
    "    plot_traj_gray_SIRD(simul_yy)\n",
    "    peak.append(max(simul_yy[:,0]))\n",
    "    day_peak.append(np.argmax(simul_yy[:,0]))\n",
    "    \n",
    "simul_all = np.array(simul_all)\n",
    "q95 = []\n",
    "q05 = []\n",
    "for i in range(eff):\n",
    "    q95.append([np.quantile(simul_all[:,i,0],0.975),np.quantile(simul_all[:,i,1],0.975),np.quantile(simul_all[:,i,2],0.975)])\n",
    "    q05.append([np.quantile(simul_all[:,i,0],0.025),np.quantile(simul_all[:,i,1],0.025),np.quantile(simul_all[:,i,2],0.025)])\n",
    "\n",
    "q95 = np.array(q95)    \n",
    "q05 = np.array(q05)  \n",
    "\n",
    "plot_traj_SIRD2(q95)\n",
    "\n",
    "plot_traj_SIRD2(q05)\n",
    "\n",
    "plot_traj_SIRD(extended_y)\n",
    "\n",
    "print(min(peak), max(peak))\n",
    "print(min(day_peak), max(day_peak))\n",
    "print(np.argmax(extended_y[:,0]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for\n",
    "#  H0: alpha >= 0\n",
    "#  H1: alpha < 0\n",
    "\n",
    "p0 = np.sum(parameters[:,3] <= 0)/parameters.shape[0]\n",
    "BF10 = p0/(1-p0)\n",
    "\n",
    "print(2*np.log(BF10))\n",
    "\n",
    "'''\n",
    "There's a slight evidence in favour of H0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps.ttest_1samp(parameters[:,3], 0, alternative='less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('parameters_veneto.npy',parameters)\n",
    "np.save('weights_veneto.npy',weights)\n",
    "np.savetxt(\"parsVeneto.csv\", parameters, delimiter=\",\")\n",
    "np.savetxt(\"trajIVeneto.csv\", trajectories2storeI, delimiter=\",\")\n",
    "np.savetxt(\"trajRVeneto.csv\", trajectories2storeR, delimiter=\",\")\n",
    "np.savetxt(\"trajDVeneto.csv\", trajectories2storeD, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parlomb = np.load('parameters_lombardia.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps.mannwhitneyu(parameters[:,3], parlomb[:,3], alternative='less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_beta_lomb = parlomb[:,0]/10*np.exp((-parlomb[:,3]*ext/1000))\n",
    "fin_beta_veneto =  parameters[:,0]/10*np.exp((-parameters[:,3]*ext/1000))\n",
    "\n",
    "print(fin_beta_veneto)\n",
    "\n",
    "sps.mannwhitneyu(fin_beta_lomb, fin_beta_veneto, alternative='greater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_beta_lomb = parlomb[:,0]/10*np.exp((-parlomb[:,3]*88/1000))\n",
    "fin_beta_veneto =  parameters[:,0]/10*np.exp((-parameters[:,3]*88/1000))\n",
    "\n",
    "print(fin_beta_veneto)\n",
    "\n",
    "sps.mannwhitneyu(fin_beta_lomb, fin_beta_veneto, alternative='greater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.sum(parameters[:,3] <= parlomb[:,3][:1655])/parameters.shape[0]\n",
    "BF10 = p0/(1-p0)\n",
    "\n",
    "print(2*np.log(BF10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
