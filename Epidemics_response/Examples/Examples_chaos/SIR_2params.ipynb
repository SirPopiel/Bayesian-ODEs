{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "#plt.switch_backend('agg')\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "keras = tf.keras\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "#from neural_ode import NeuralODE\n",
    "\n",
    "import sys\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "target_dir = os.path.sep.join(current_dir.split(os.path.sep)[:])\n",
    "sys.path.append(os.path.join(os.path.dirname(target_dir),'../eABCSMC'))\n",
    "sys.path.append(os.path.join(os.path.dirname(target_dir),'../Epidemics_response/Data'))\n",
    "sys.path.append(target_dir)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR(z, t, beta, gamma):\n",
    "\n",
    "    S, I, R = z\n",
    "\n",
    "    dS = - beta * I * S / N \n",
    "    dI = beta * I * S / N - gamma * I\n",
    "    dR = gamma * I \n",
    "\n",
    "    dzdt = [dS, dI, dR]\n",
    "    return dzdt\n",
    "\n",
    "def SIR_diffparam(z, t, beta, beta2, gamma, gamma2, errato = 0):\n",
    "\n",
    "    S, I, R = z\n",
    "\n",
    "    dS = - beta * I * S / N \n",
    "    dI = beta2 * I * S / N - gamma * I\n",
    "    dR = gamma2 * I \n",
    "\n",
    "    dzdt = [dS, dI, dR]\n",
    "    return dzdt\n",
    "\n",
    "# adattare il modello tempo-variabile -> così da considerare lockdown ecc? -> o con alcuni threshold\n",
    "\n",
    "def plot_traj(trajectories, width = 1.):\n",
    "    x1 = trajectories[:,0]\n",
    "    x2 = trajectories[:,1]\n",
    "    x3 = trajectories[:,2]\n",
    "    s = plt.plot(x1, linewidth = width, label = 'Susceptible')\n",
    "    i = plt.plot(x2, linewidth = width, label = 'Infected')\n",
    "    r = plt.plot(x3, linewidth = width, label = 'Removed')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.title('Simulation with R0 = %f' %R0 )\n",
    "    \n",
    "    \n",
    "def plot_traj_gray(trajectories, width = 1.):\n",
    "    x1 = trajectories[:,0]\n",
    "    x2 = trajectories[:,1]\n",
    "    x3 = trajectories[:,2]\n",
    "    s = plt.plot(x1, linewidth = width, color = 'lightgray')\n",
    "    i = plt.plot(x2, linewidth = width, color = 'lightgray')\n",
    "    r = plt.plot(x3, linewidth = width, color = 'lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oop con ereditarietà?\n",
    "\n",
    "def SEIR(z, t, beta, gamma, lamb, mu, a):\n",
    "\n",
    "    S, E, I, R = z\n",
    "\n",
    "    dS = - beta * I * S / N \n",
    "    dE = beta * I * S / N - a * E\n",
    "    dI = a * E - gamma  * I\n",
    "    dR = gamma * I \n",
    "\n",
    "    dzdt = [dS, dE, dI, dR]\n",
    "    return dzdt\n",
    "\n",
    "def plot_traj_SEIR(trajectories, width = 1.):\n",
    "    x1 = trajectories[:,0]\n",
    "    x2 = trajectories[:,1]\n",
    "    x3 = trajectories[:,2]\n",
    "    x4 = trajectories[:,3]\n",
    "    \n",
    "    s = plt.plot(x1, linewidth = width, label = 'Susceptible')\n",
    "    r = plt.plot(x2, linewidth = width, label = 'Exposed')\n",
    "    i = plt.plot(x3, linewidth = width, label = 'Infected')\n",
    "    r = plt.plot(x4, linewidth = width, label = 'Removed')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.title('Simulation with R0 = %f' %(a*beta/gamma) )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 100\n",
    "\n",
    "batch_time = 20  # tempo delle batches\n",
    "niters = 500\n",
    "#batch_size = 84  # dimensione delle batches\n",
    "batch_size = data_size - batch_time - 1\n",
    "\n",
    "N = 60000\n",
    "infected_0 = 100\n",
    "beta = 0.3 # farli time evolving?\n",
    "gamma = 0.1\n",
    "\n",
    "R0 = beta/gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_grid = np.linspace(0, data_size-1, data_size)  # uniformly spaced data? -> even though advantage is learning with not uniformly spaced data\n",
    "z0 = [N - infected_0, infected_0, 0] # initial conditions\n",
    "true_yy = odeint(SIR, z0, t_grid, args=(beta, gamma))  # potrebbe aver senso tenerli in memoria se è lento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj(true_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_y = true_yy.copy()\n",
    "\n",
    "\n",
    "\n",
    "from stochastic.processes.continuous import FractionalBrownianMotion\n",
    "\n",
    "fbm = FractionalBrownianMotion(hurst=0.2, t= data_size)\n",
    "# in questo modo è autocorrelato negativamente -> giorno da tanti tamponi seguito da giorno da pochi tamponi\n",
    "\n",
    "noise = abs(N/200 * fbm.sample(data_size-1))\n",
    "noise_2 = abs(N/200 * fbm.sample(data_size-1))\n",
    "\n",
    "\n",
    "true_y[:,0] = abs(true_y[:,0] - noise)        \n",
    "true_y[:,1] = abs(true_y[:,1] + noise - noise_2)\n",
    "true_y[:,2] = N - true_y[:,0] - true_y[:,1]\n",
    "\n",
    "# problema in realtà è che i suscettibili e i rimossi possono crescere/decrescere\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_y = true_yy\n",
    "plot_traj(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_s = np.std(true_y[:,0:1])  \n",
    "sigma_i = np.std(true_y[:,1:2]) \n",
    "sigma_r = np.std(true_y[:,2:3]) \n",
    "\n",
    "true_y[:, 0:1] = true_y[:, 0:1]/sigma_s\n",
    "true_y[:, 1:2] = true_y[:, 1:2]/sigma_i\n",
    "true_y[:, 2:3] = true_y[:, 2:3]/sigma_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    \"\"\"Returns initial point and last point over sampled frament of trajectory\"\"\"\n",
    "    starts = np.random.choice(np.arange(data_size - batch_time - 1, dtype=np.int64), batch_size, replace=False)\n",
    "    # This randomly chooses from {0, 1, ... , data_size - batch_time - 1}, batch_size different elements\n",
    "    batch_y0 = true_y[starts] \n",
    "    batch_yN = true_y[starts + batch_time]\n",
    "    # The function returns a tensor composed by some y0 and the respective yN,\n",
    "    # being y0 + DeltaT.\n",
    "    return tf.cast(batch_y0, dtype=tf.float32), tf.cast(batch_yN, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_param = 5 # Number of parameters\n",
    "para_num = num_param\n",
    "\n",
    "t0 = t_grid[:batch_time][0]  # t0 = first element of t_grid\n",
    "t1 = t_grid[:batch_time][-1]  # t1 = the element of t_grid at batch_time\n",
    "t_in = np.linspace(t0, t1, 10)  # The time grid between t0 and t1\n",
    "\n",
    "batch_y0, batch_yN = get_batch()  # Returns the first and the last y observed for each batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_ode import NeuralODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters_pre = 500  # Number of iterations of the preconditioner\n",
    "\n",
    "class ODEModel_pre(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ODEModel_pre, self).__init__()\n",
    "        self.Weights = tf.Variable(tf.random.normal([num_param, 1], dtype=tf.float32)*0.01, dtype=tf.float32)\n",
    "    # Initializer: assign normally distributed random weights which are very close to zero\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        t, y = inputs\n",
    "        h = y\n",
    "        S = h[:, 0:1] * sigma_s\n",
    "        I = h[:, 1:2] * sigma_i\n",
    "        R = h[:, 2:3] * sigma_r\n",
    "        \n",
    "\n",
    "        # ma che sintassi di merda -> facciamolo adattativo\n",
    "\n",
    "        p1 = self.Weights[0]\n",
    "        p2 = self.Weights[1]\n",
    "        p3 = self.Weights[2]\n",
    "        p4 = self.Weights[3]\n",
    "        p5 = self.Weights[4]\n",
    "\n",
    "        dS = - p1 * I * S / N + p5 * R \n",
    "        dI = p2 * I * S / N - p3 * I \n",
    "        dR = p4 * I - p5 * R\n",
    "        \n",
    "        h_out = tf.concat([dS/sigma_s, dI/sigma_i, dR/sigma_r], 1)\n",
    "        return h_out\n",
    "\n",
    "\n",
    "model_pre = ODEModel_pre()  \n",
    "neural_ode_pre = NeuralODE(model_pre, t_in) \n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(3e-2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_gradients_and_update_pre(batch_y0, batch_yN):\n",
    "    \"\"\"Takes start positions (x0, y0) and final positions (xN, yN)\"\"\"\n",
    "    pred_y = neural_ode_pre.forward(batch_y0)  # Predict y using Runge-Kutta 4 for each y0 in batch_y0\n",
    "    with tf.GradientTape() as g_pre:\n",
    "        g_pre.watch(pred_y)\n",
    "        loss = tf.reduce_mean(input_tensor=(pred_y - batch_yN)**2) + tf.reduce_sum(input_tensor=tf.abs(model_pre.trainable_weights[0]))\n",
    "        # This step is computing the loss function\n",
    "    dLoss = g_pre.gradient(loss, pred_y)  # Here we compute the gradient of the loss function\n",
    "    h_start, dfdh0, dWeights = neural_ode_pre.backward(pred_y, dLoss)  # Here we compute the dWeights\n",
    "    optimizer.apply_gradients(zip(dWeights, model_pre.weights))  # Here we update the weights\n",
    "    return loss, dWeights\n",
    "\n",
    "#parameters_pre = np.zeros((para_num, niters_pre))\n",
    "parameters_pre = np.zeros((para_num, 1))\n",
    "\n",
    "for step in tqdm(range(niters_pre)):\n",
    "    loss, dWeights = compute_gradients_and_update_pre(batch_y0, batch_yN)\n",
    "    #parameters_pre[:,step] = np.reshape(model_pre.trainable_weights[0].numpy(),(5,))\n",
    "    parameters_pre = model_pre.trainable_weights[0].numpy()\n",
    "print(parameters_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weight = parameters_pre  # We initialize the weights with the parameters found in preconditioning\n",
    "print(initial_weight.shape, \"here\")\n",
    "\n",
    "\n",
    "class ODEModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ODEModel, self).__init__()\n",
    "        self.Weights = tf.Variable(tf.random.normal([num_param, 1], dtype=tf.float32)*0.01, dtype=tf.float32)\n",
    "        # Initializer, initializes the weight to normal random variables with sd = 0.01\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        t, y = inputs\n",
    "        h = y\n",
    "        S = h[:, 0:1] * sigma_s\n",
    "        I = h[:, 1:2] * sigma_i\n",
    "        R = h[:, 2:3] * sigma_r\n",
    "        \n",
    "\n",
    "        # ma che sintassi di merda -> facciamolo adattativo\n",
    "\n",
    "        p1 = self.Weights[0]\n",
    "        p2 = self.Weights[1]\n",
    "        p3 = self.Weights[2]\n",
    "        p4 = self.Weights[3]\n",
    "        p5 = self.Weights[4]\n",
    "\n",
    "        dS = - p1 * I * S / N + p5 * R \n",
    "        dI = p2 * I * S / N - p3 * I \n",
    "        dR = p4 * I - p5 * R\n",
    "        \n",
    "        h_out = tf.concat([dS/sigma_s, dI/sigma_i, dR/sigma_r], 1)\n",
    "        return h_out\n",
    "\n",
    "\n",
    "model = ODEModel()\n",
    "neural_ode = NeuralODE(model, t=t_in)  # We assign to NeuralODE the just created model and the time grid  between t0 and t1\n",
    "\n",
    "@tf.function\n",
    "def compute_gradients_and_update(batch_y0, batch_yN): \n",
    "    \"\"\"Takes start positions (x0, y0) and final positions (xN, yN)\"\"\"\n",
    "    pred_y = neural_ode.forward(batch_y0)  # This finds the predicted yNs\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(pred_y)\n",
    "        loss = tf.reduce_sum(input_tensor=(pred_y - batch_yN)**2)  # This creates the loss function\n",
    "\n",
    "    dLoss = g.gradient(loss, pred_y)  # This computes the gradient of the loss function\n",
    "    h_start, dfdh0, dWeights = neural_ode.backward(pred_y, dLoss)  # This applies the gradient descent to find\n",
    "    # the updates for the weights\n",
    "\n",
    "    return loss, dWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kinetic_energy(V, loggamma_v, loglambda_v):\n",
    "    q = (np.sum(-V**2)/mom_theta - loggamma_v**2/mom_gamma - loglambda_v**2/mom_lambda)/2.0\n",
    "    return q\n",
    "\n",
    "\n",
    "def compute_gradient_param(dWeights, loggamma, loglambda, batch_size, para_num):\n",
    "    WW = model.trainable_weights[0].numpy()\n",
    "    dWeights = np.exp(loggamma)/2.0 * dWeights + np.exp(loglambda) * np.sign(WW - w_means)\n",
    "    return dWeights\n",
    "\n",
    "\n",
    "def compute_gradient_hyper(loss, weights, loggamma, loglambda, batch_size, para_num):\n",
    "    grad_loggamma = np.exp(loggamma) * (loss/2.0 + 1.0) - (batch_size/2.0 + 1.0)\n",
    "    grad_loglambda = np.exp(loglambda) * (np.sum(np.abs(weights - w_means)) + 1.0) - (para_num + 1.0)\n",
    "    # This somehow computes the gradient of the hyper parameters in order to update them from step to step\n",
    "\n",
    "    return grad_loggamma, grad_loglambda\n",
    "\n",
    "\n",
    "def compute_Hamiltonian(loss, weights, loggamma, loglambda, batch_size, para_num):\n",
    "    H = np.exp(loggamma)*(loss/2.0 + 1.0) + np.exp(loglambda)*(np.sum(np.abs(weights - w_means)) + 1.0) \\\n",
    "             - (batch_size/2.0 + 1.0) * loggamma - (para_num + 1.0) * loglambda\n",
    "    return H\n",
    "\n",
    "def leap_frog(v_in, w_in, loggamma_in, loglambda_in, loggamma_v_in, loglambda_v_in):\n",
    "    # Leap frog step for Hamiltonian Monte Carlo\n",
    "\n",
    "    model.trainable_weights[0].assign(w_in)\n",
    "    v_new = v_in\n",
    "    loggamma_v_new = loggamma_v_in\n",
    "    loglambda_v_new = loglambda_v_in\n",
    "\n",
    "    loggamma_new = loggamma_in\n",
    "    loglambda_new = loglambda_in\n",
    "    w_new = w_in\n",
    "\n",
    "    for m in range(L):\n",
    "        loss, dWeights = compute_gradients_and_update(batch_y0, batch_yN) # evaluate the gradient\n",
    "\n",
    "        dWeights = np.asarray(dWeights[0])  # make the gradient to be numpy array\n",
    "        dWeights = compute_gradient_param(dWeights, loggamma_new, loglambda_new, batch_size, para_num)\n",
    "        grad_loggamma, grad_loglambda = compute_gradient_hyper(loss, w_new, loggamma_new, loglambda_new, batch_size, para_num)\n",
    "\n",
    "        loggamma_v_new = loggamma_v_new - epsilon/2*grad_loggamma  # Computation of new velocities\n",
    "        loglambda_v_new = loglambda_v_new - epsilon/2*grad_loglambda  # 2nd component of velocity\n",
    "        v_new = v_new - epsilon/2*(dWeights)  # 3rd component of velocity\n",
    "        w_new = model.trainable_weights[0].numpy() + epsilon / mom_theta * v_new  # updating of x_1\n",
    "        model.trainable_weights[0].assign(w_new)  # Assignment\n",
    "        loggamma_new = loggamma_new + epsilon / mom_gamma * loggamma_v_new  # Updating of x_2\n",
    "        loglambda_new = loglambda_new + epsilon / mom_lambda * loglambda_v_new  # Updating of x_3\n",
    "\n",
    "        # Second half of the leap frog\n",
    "        loss, dWeights = compute_gradients_and_update(batch_y0, batch_yN)\n",
    "        dWeights = np.asarray(dWeights[0])\n",
    "        dWeights = compute_gradient_param(dWeights, loggamma_new, loglambda_new, batch_size, para_num)\n",
    "        grad_loggamma, grad_loglambda = compute_gradient_hyper(loss, w_new, loggamma_new, loglambda_new, batch_size, para_num)\n",
    "\n",
    "        v_new = v_new - epsilon/2*(dWeights)  # Updating of new velocities\n",
    "        loggamma_v_new = loggamma_v_new - epsilon/2*grad_loggamma  # Updating of new velocities\n",
    "        loglambda_v_new = loglambda_v_new - epsilon/2*grad_loglambda  # Updating of new velocities\n",
    "\n",
    "    return v_new, w_new, loggamma_new, loglambda_new, loggamma_v_new, loglambda_v_new\n",
    "\n",
    "\n",
    "neural_ode_test = NeuralODE(model, t=t_grid[0:data_size:20])\n",
    "parameters = np.zeros((niters, para_num))  # book keeping the parameters\n",
    "loggammalist = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "loglambdalist = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "loglikelihood = np.zeros((niters, 1))  # book keeping the loggamma\n",
    "mom_theta = 0.2 # Momentum for the Hamiltonian Monte Carlo for theta parameters\n",
    "mom_lambda = 0.1  # Momentum for the Hamiltonian Monte Carlo for lambda parameters\n",
    "mom_gamma = 0.1  # Momentum for the Hamiltonian Monte Carlo for gamma parameters\n",
    "L = 30  # leap frog step number\n",
    "epsilon = 0.001  # leap frog step size  0.001\n",
    "epsilon_max = 0.0002    # max 0.001  0.0002\n",
    "epsilon_min = 0.0002    # max 0.001  0.0002\n",
    "acc_rate = 0  # Used to compute and display the acceptance rate of the candidates\n",
    "w_means = np.array([0, 0, 0, 0, 0], dtype=np.float32)  # A priori means of parameters of the model\n",
    "w_means = w_means.reshape(5, 1)\n",
    "\n",
    "def compute_epsilon(step):\n",
    "    # This will compute the epsilon to use in the leapfrog which is different for every step. It decreases\n",
    "    # with the steps increasing in number\n",
    "    coefficient = np.log(epsilon_max/epsilon_min)\n",
    "    return epsilon_max * np.exp(- step * coefficient / niters)\n",
    "\n",
    "\n",
    "# initial weight\n",
    "w_temp = initial_weight  # The one we found from the preconditioner\n",
    "print(\"initial_w\", w_temp)\n",
    "loggamma_temp = 4. + np.random.normal()\n",
    "loglambda_temp = np.random.normal()\n",
    "\n",
    "model.trainable_weights[0].assign(w_temp)  # We assign to the weights of the model, the ones we found through\n",
    "# the pre conditioner. Remember that the initial weights set by the initializer were random distributed according\n",
    "# to a Gaussian with 0 mean and 0.01 sd.\n",
    "loss_original, _ = compute_gradients_and_update(batch_y0, batch_yN)  # Compute the initial Hamiltonian\n",
    "\n",
    "loggamma_temp = np.log(batch_size / loss_original)  # We define an initial guess for loggamma ?? Why defined as such?\n",
    "\n",
    "print(\"This is the initial guess of log(gamma)\", loggamma_temp, \"with loss\", loss_original)\n",
    "if loggamma_temp > 6.:\n",
    "    loggamma_temp = 6.\n",
    "    epsilon_max = 0.0002\n",
    "    epsilon_min = 0.0002\n",
    "\n",
    "# training steps\n",
    "for step in tqdm(range(niters)):\n",
    "\n",
    "    epsilon = compute_epsilon(step)  # Compute the adaptive epsilon for the steps\n",
    "    v_initial = np.sqrt(mom_theta)*np.random.randn(para_num, 1)  # initialize the velocity\n",
    "    loggamma_v_initial = np.sqrt(mom_gamma)*np.random.normal()\n",
    "    loglambda_v_initial = np.sqrt(mom_lambda)*np.random.normal()\n",
    "\n",
    "    loss_initial, _ = compute_gradients_and_update(batch_y0, batch_yN)  # compute the initial Hamiltonian\n",
    "    # This line uses the weights of the preconditioner (see line 303) to compute the loss function with those\n",
    "    loss_initial = compute_Hamiltonian(loss_initial, w_temp, loggamma_temp, loglambda_temp, batch_size, para_num)\n",
    "    # Then it computes the Hamiltonian\n",
    "\n",
    "    v_new, w_new, loggamma_new, loglambda_new, loggamma_v_new, loglambda_v_new = \\\n",
    "                            leap_frog(v_initial, w_temp, loggamma_temp, loglambda_temp, loggamma_v_initial, loglambda_v_initial)\n",
    "\n",
    "    # Then the leapfrog is applied in order to update the parameters and the hyper parameters and to further\n",
    "    # optimize the weights estimation (L steps of leapfrog at a time)\n",
    "\n",
    "    # compute the final Hamiltonian, using the updated sampled weights found through leapfrog\n",
    "    loss_finial, _ = compute_gradients_and_update(batch_y0, batch_yN)\n",
    "    loss_finial = compute_Hamiltonian(loss_finial, w_new, loggamma_new, loglambda_new, batch_size, para_num)\n",
    "\n",
    "    # making decisions\n",
    "    p_temp = np.exp(-loss_finial + loss_initial + \\\n",
    "                    kinetic_energy(v_new, loggamma_v_new, loglambda_v_new) - kinetic_energy(v_initial, loggamma_v_initial, loglambda_v_initial))\n",
    "\n",
    "    p = min(1, p_temp)\n",
    "    p_decision = np.random.uniform()\n",
    "    if p > p_decision:\n",
    "        parameters[step:step+1, :] = np.transpose(w_new)  # Parameters are updated\n",
    "        w_temp = w_new\n",
    "        loggammalist[step, 0] = loggamma_new\n",
    "        loglambdalist[step, 0] = loglambda_new\n",
    "        loglikelihood[step, 0] = loss_finial\n",
    "        loggamma_temp = loggamma_new\n",
    "        loglambda_temp = loglambda_new\n",
    "        acc_rate += 1\n",
    "    else:\n",
    "        parameters[step:step+1, :] = np.transpose(w_temp)  # New parameters are not updated\n",
    "        model.trainable_weights[0].assign(w_temp)\n",
    "        loggammalist[step, 0] = loggamma_temp\n",
    "        loglambdalist[step, 0] = loglambda_temp\n",
    "        loglikelihood[step, 0] = loss_initial\n",
    "\n",
    "    #print('Probability of acceptance: ', p)\n",
    "    #print('Accepted: ', p > p_decision)\n",
    "    print(parameters[step:step+1, :])\n",
    "\n",
    "print('Acceptance rate: ', acc_rate/niters)\n",
    "\n",
    "np.save('parameters', parameters)  # The Monte Carlo chain of the parameters\n",
    "np.save('loggammalist', loggammalist)  # The Monte Carlo chain of loggamma\n",
    "np.save('loglikelihood', loglikelihood)  # The Monte Carlo chain of losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics import tsaplots\n",
    "tsaplots.plot_acf(parameters[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loglambdalist)\n",
    "plt.plot(loggammalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "g = sns.PairGrid(pd.DataFrame(parameters))\n",
    "g.map_upper(sns.histplot)\n",
    "g.map_lower(sns.kdeplot, fill=True)\n",
    "g.map_diag(sns.histplot, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_grid = np.linspace(0, data_size, data_size)  # uniformly spaced data? -> even though advantage is learning with not uniformly spaced data\n",
    "z0 = [N - infected_0, infected_0, 0] # initial conditions\n",
    "\n",
    "for j in tqdm(range(parameters.shape[0])):\n",
    "    #simul_yy =  odeint(SIR_diffparam, z0, t_grid, args=tuple([np.median(parameters[:,i]) for i in range(parameters.shape[1])]))\n",
    "    simul_yy =  odeint(SIR_diffparam, z0, t_grid, args=tuple(parameters[j]))\n",
    "    plot_traj_gray(simul_yy)\n",
    "\n",
    "true_yy = odeint(SIR, z0, t_grid, args=(beta, gamma))  # potrebbe aver senso tenerli in memoria se è lento\n",
    "plot_traj(true_yy)\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon, mannwhitneyu, kruskal\n",
    "\n",
    "kruskal(parameters[:,0],parameters[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.ess(parameters[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(parameters[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.hdi(parameters[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.geweke(parameters[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_mcse(parameters[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "\n",
    "# - simulation with only two parameters\n",
    "# - go back to noisy simulation\n",
    "# - thinning\n",
    "# - weighted noise\n",
    "# - CI for intensive therapy\n",
    "# - sensitivity analysis!\n",
    "\n",
    "# - understand why bayesian\n",
    "# - sliding window for parameters estimation?\n",
    "# - non observable states' estimation (through t-1?)\n",
    "# - different dynamical systems interactions \n",
    "\n",
    "# - batch does not really make sense\n",
    "# -> we always overestimate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
